{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TypDOiTPbjE"
   },
   "source": [
    "# LangChain 기능 소개\n",
    "\n",
    "- 1. 프롬프트 템플릿과 로더를 사용하여 체인 구성  \n",
    "- 2. Runnable과 LangChain 표현 언어  \n",
    "- 3. RAG 체인 구축  \n",
    "- 4. 체인이 달린 도구 사용  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOUiSLMxPzZY"
   },
   "source": [
    "## LangChain 개요\n",
    "\n",
    "LangChain으로 개발된 애플리케이션은 일반적으로 세 가지 범주로 나뉩니다.\n",
    "\n",
    "- 챗봇 : LLM을 이용하여 보다 지능적인 마케팅, 고객 지원, 교육 상호작용 구현.\n",
    "- 검색 증강 생성(RAG) Q&A : 대용량 문서 요약, 데이터 분석, 외부 소스를 참조하여 코드 생성에 사용.\n",
    "- 에이전트 시스템은 다중 에이전트 설정과 인간 상호작용을 포함하며 복잡한 워크플로우를 위해 LangGraph를 활용. 공급망 관리 및 운영 최적화와 같은 분야에 적용 가능.\n",
    "\n",
    "LangChain은 자연어를 실행 가능한 프로그램으로 변환하는 파이프라인을 생성할 수 있게 합니다. 이러한 체인을 활용함으로써 사용자는 자연어를 입력하고 보고서, 분석 또는 컴퓨터 프로그램과 같은 출력을 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNdbWOsusEgx",
    "outputId": "c8df9de3-aabf-459f-bcfa-fe09c692bfbf"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "# python-dotenv \\\n",
    "# langchain \\\n",
    "# langchain-community \\\n",
    "# openai \\\n",
    "# anthropic \\\n",
    "# langchain-openai \\\n",
    "# langchain-anthropic \\\n",
    "# langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXRwb1YcsW1j",
    "outputId": "7e5cff59-b6f5-446d-e3cc-93891014f5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env 파일에서 API 키를 로드\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEi1gjmpW8ab"
   },
   "source": [
    "## LLM 연결\n",
    "\n",
    "OpenAI 및 Anthropic API에 연결합니다. 원하는 모델을 지정하여 ChatOpenAI 및 ChatAnthropic 클래스의 인스턴스를 만듭니다. llm_claude3 인스턴스를 사용하여 간단한 쿼리를 호출하여 설정을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jSY2uUq8ssCp"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm_gpt4 = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "llm_claude3 = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\")\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "NFWMvMRitGX-",
    "outputId": "9eff0799-8467-4610-ee63-d3b945657364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 자연어 처리(NLP)와 관련된 다양한 작업을 수행하기 위해 설계된 프레임워크입니다. 주로 대화형 AI 및 언어 모델을 기반으로 한 애플리케이션 개발에 유용합니다. LangChain의 주요 목표는 언어 모델을 보다 쉽게 통합하고 활용할 수 있도록 다양한 도구와 구성 요소를 제공하는 것입니다.\n",
      "\n",
      "LangChain의 주요 기능은 다음과 같습니다:\n",
      "\n",
      "1. **모듈화**: LangChain은 여러 구성 요소로 나뉘어 있어, 필요한 기능만 선택하여 사용할 수 있습니다. 예를 들어, 텍스트 생성, 질문 응답, 대화 관리 등 다양한 기능을 모듈화된 형태로 제공합니다.\n",
      "\n",
      "2. **다양한 언어 모델 지원**: LangChain은 OpenAI의 GPT, Hugging Face의 Transformers 등 다양한 언어 모델을 지원하여, 사용자가 원하는 모델을 쉽게 선택하고 사용할 수 있게 합니다.\n",
      "\n",
      "3. **연결성**: LangChain은 외부 데이터 소스와의 연결이 용이하도록 설계되어 있습니다. 이를 통해 사용자는 데이터베이스, API 및 기타 외부 서비스와 통합하여 더 복잡한 애플리케이션을 개발할 수 있습니다.\n",
      "\n",
      "4. **상태 관리**: 대화형 애플리케이션에서 사용자와의 대화 상태를 관리할 수 있는 기능을 제공하여, 더욱 자연스럽고 일관된 대화를 가능하게 합니다.\n",
      "\n",
      "5. **확장성**: LangChain은 사용자가 쉽게 커스터마이즈하거나 확장할 수 있도록 설계되어, 특정 요구사항에 맞는 기능을 추가할 수 있습니다.\n",
      "\n",
      "LangChain은 특히 챗봇, 가상 비서, 고객 지원 시스템 등 다양한 대화형 AI 애플리케이션 개발에 매우 유용하며, 개발자들이 더 쉽게 언어 모델을 활용할 수 있도록 돕습니다.\n"
     ]
    }
   ],
   "source": [
    "print(llm_gpt4.invoke(\"한국어로 LangChain 에 대해서 설명해줘\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "q9Y-0gAhxCx2",
    "outputId": "7c94fbd4-7612-4799-9ab7-6c84c7831fc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 대규모 언어 모델(LLM)을 사용하여 애플리케이션을 개발하기 위한 프레임워크입니다. 주요 특징과 구성 요소를 설명하겠습니다:\n",
      "\n",
      "주요 특징:\n",
      "1. 모듈성: 다양한 LLM과 도구들을 쉽게 통합하고 조합할 수 있음\n",
      "2. 체인화: 복잡한 작업을 작은 단위로 나누어 처리 가능\n",
      "3. 확장성: 다양한 외부 도구와 서비스를 쉽게 연동 가능\n",
      "\n",
      "주요 구성 요소:\n",
      "\n",
      "1. Models (모델)\n",
      "- LLM: OpenAI GPT, Anthropic Claude 등\n",
      "- 임베딩: 텍스트를 벡터로 변환\n",
      "- 채팅 모델: 대화형 AI 모델\n",
      "\n",
      "2. Prompts (프롬프트)\n",
      "- 프롬프트 템플릿\n",
      "- 프롬프트 최적화\n",
      "- 예시 관리\n",
      "\n",
      "3. Chains (체인)\n",
      "- 여러 컴포넌트를 연결\n",
      "- 순차적 작업 처리\n",
      "- 복잡한 워크플로우 구현\n",
      "\n",
      "4. Memory (메모리)\n",
      "- 대화 기록 저장\n",
      "- 컨텍스트 관리\n",
      "- 상태 유지\n",
      "\n",
      "5. Agents (에이전트)\n",
      "- 자동화된 의사결정\n",
      "- 도구 사용\n",
      "- 태스크 실행\n",
      "\n",
      "활용 사례:\n",
      "1. 문서 분석 및 요약\n",
      "2. 챗봇 개발\n",
      "3. 데이터 처리 자동화\n",
      "4. 질의응답 시스템\n",
      "5. 코드 생성 및 분석\n",
      "\n",
      "장점:\n",
      "1. 개발 시간 단축\n",
      "2. 코드 재사용성 향상\n",
      "3. 확장 가능한 구조\n",
      "4. 다양한 통합 옵션\n",
      "\n",
      "단점:\n",
      "1. 학습 곡선이 있음\n",
      "2. 문서화가 아직 발전 중\n",
      "3. 일부 기능의 안정성 개선 필요\n",
      "\n",
      "이러한 특징들로 인해 LangChain은 AI 기반 애플리케이션 개발에 매우 유용한 도구로 평가받고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(llm_claude3.invoke(\"한국어로 LangChain 에 대해서 설명해줘\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "aiKZpBOHnYJn",
    "outputId": "d2201765-56c3-49ef-a9de-3c45f9b0be74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 대규모 언어 모델(LLM)을 사용하여 애플리케이션을 구축하기 위한 프레임워크입니다.  간단히 말해, LLM의 능력을 최대한 활용하여 더욱 강력하고 복잡한 작업을 수행할 수 있도록 도와주는 도구라고 생각하면 됩니다.\n",
      "\n",
      "LangChain은 다음과 같은 핵심 구성 요소를 제공합니다:\n",
      "\n",
      "* **모델:** OpenAI, Hugging Face 등 다양한 LLM 제공 업체와 통합되어 원하는 모델을 선택하고 사용할 수 있습니다.\n",
      "* **프롬프트 템플릿:** LLM에 효과적인 질문(프롬프트)을 작성하는 것은 매우 중요합니다. LangChain은 프롬프트를 체계적으로 관리하고, 변수를 사용하여 동적으로 프롬프트를 생성하는 템플릿 기능을 제공합니다.  이를 통해 일관성 있고 효율적인 프롬프트를 생성하여 LLM의 성능을 향상시킬 수 있습니다.\n",
      "* **체인:** 여러 LLM 호출을 순차적으로 연결하여 복잡한 작업을 처리할 수 있습니다. 예를 들어, 텍스트 요약 후 번역, 질의응답 후 추가 정보 검색 등의 작업을 하나의 체인으로 구성할 수 있습니다.\n",
      "* **인덱스:** 외부 데이터 소스(예: 문서, 데이터베이스)에 접근하고 LLM과 연결하여 더 풍부하고 정확한 결과를 얻을 수 있도록 도와줍니다.  LLM은 학습 데이터에 한정된 지식만 가지고 있지만, LangChain을 통해 외부 데이터를 활용하면 최신 정보나 특정 분야의 전문 지식을 활용한 작업이 가능해집니다.\n",
      "* **메모리:** LLM 호출 간의 대화 기록을 저장하고 관리하여 맥락을 유지할 수 있습니다.  챗봇처럼 이전 대화 내용을 기억하고 활용하여 더 자연스럽고 일관된 응답을 생성하는 데 필수적인 기능입니다.\n",
      "* **콜백:** LLM과의 상호작용 과정을 모니터링하고 제어할 수 있는 기능을 제공합니다.  실행 과정을 추적하거나 중간 결과를 확인하여 디버깅 및 성능 최적화에 도움을 줍니다.\n",
      "\n",
      "\n",
      "LangChain을 사용하면 다음과 같은 애플리케이션을 구축할 수 있습니다:\n",
      "\n",
      "* **챗봇:** 자연스러운 대화를 통해 사용자의 질문에 답변하고 다양한 작업을 수행하는 챗봇을 개발할 수 있습니다.\n",
      "* **질의응답:** 특정 문서나 데이터베이스에서 정보를 검색하고 질문에 대한 답변을 제공하는 시스템을 구축할 수 있습니다.\n",
      "* **요약:** 긴 텍스트를 요약하거나 핵심 내용을 추출하는 애플리케이션을 개발할 수 있습니다.\n",
      "* **코드 생성:** 자연어 설명을 기반으로 코드를 생성하는 도구를 개발할 수 있습니다.\n",
      "\n",
      "\n",
      "LangChain은 Python 및 JavaScript를 지원하며, 활발한 커뮤니티와 풍부한 문서를 통해 개발자들에게 다양한 지원을 제공합니다.  LLM을 활용한 애플리케이션 개발에 관심이 있다면 LangChain을 통해 더욱 효율적이고 강력한 애플리케이션을 구축할 수 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_gemini.invoke(\"한국어로 LangChain 에 대해서 설명해줘\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages\n",
    "- Message: AI(인공지능) 모델이 생성한 메시지를 나타냅니다.  \n",
    "예를 들어, AI 모델이 사용자의 질문에 대해 응답을 생성할 때 이 클래스가 사용됩니다.\n",
    "\n",
    "- manMessage: 사람(사용자)이 생성한 메시지를 나타냅니다.  \n",
    "사용자가 입력한 텍스트나 질문 등이 여기에 해당됩니다.\n",
    "\n",
    "- stemMessage: AI 행동을 프라이밍하기 위한 메시지.\n",
    "시스템 메시지는 일반적으로 일련의 입력 메시지 중 첫 번째로 전달됩니다. 일반적으로 대화의 흐름을 제어하거나 특정 지침을 제공하기 위해 사용됩니다.\n",
    "\n",
    "** 프라이밍(Priming)은 시스템 메시지에서 특정 행동이나 반응을 유도하기 위해 미리 정보를 제공하거나 맥락을 설정하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "마치 다섯살 먹은 어린아이에게 설명하듯이 한국어로 쉽게 설명해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "LangChain이 무엇인가요?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYhhibXZSGTF",
    "outputId": "3d276aa8-39a2-4b85-b9aa-5bb43729783b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain은 컴퓨터가 사람처럼 말을 하거나 글을 쓸 수 있도록 도와주는 특별한 도구예요. 마치 친구와 대화하듯이, 컴퓨터가 우리 질문에 대답을 잘 할 수 있게 해주는 방법들이 모여 있는 거죠. \\n\\n예를 들어, 우리가 좋아하는 동물에 대해 질문을 하면, LangChain은 그 질문을 이해하고, 우리에게 재미있는 대답을 해줄 수 있도록 도와줘요. 그래서 우리가 컴퓨터와 더 재미있고 똑똑하게 대화를 할 수 있게 해주는 친구 같은 존재예요!', response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 43, 'total_tokens': 171, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'stop', 'logprobs': None}, id='run-36557680-41ce-47ae-ba26-1edca9d906c3-0', usage_metadata={'input_tokens': 43, 'output_tokens': 128, 'total_tokens': 171})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# system과 human/user 메시지를 이용한 기본 요청\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]\n",
    "\n",
    "response = llm_gpt4.invoke(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "AK7Xe8wnSQGa",
    "outputId": "c314481f-1ecb-4337-bf3c-08c274701b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 컴퓨터가 사람처럼 말을 하거나 글을 쓸 수 있도록 도와주는 특별한 도구예요. 마치 친구와 대화하듯이, 컴퓨터가 우리 질문에 대답을 잘 할 수 있게 해주는 방법들이 모여 있는 거죠. \n",
      "\n",
      "예를 들어, 우리가 좋아하는 동물에 대해 질문을 하면, LangChain은 그 질문을 이해하고, 우리에게 재미있는 대답을 해줄 수 있도록 도와줘요. 그래서 우리가 컴퓨터와 더 재미있고 똑똑하게 대화를 할 수 있게 해주는 친구 같은 존재예요!\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4i2vpQctaFA",
    "outputId": "03c03817-31c3-4faf-df60-2f2940748bcb"
   },
   "outputs": [],
   "source": [
    "# import textwrap\n",
    "\n",
    "# #텍스트를 지정된 너비에 맞게 줄 바꿈하여 하나의 문자열로 반환\n",
    "# answer = textwrap.fill(response.content, width=100)\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtMb5t7P0tRI",
    "outputId": "003b57d1-36a4-41e1-ad93-38d0034a1d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 ChatGPT 같은 AI를 더 쉽게 사용할 수 있게 도와주는 도구예요.\n",
      "\n",
      "예를 들어, 장난감 블록을 가지고 놀 때처럼 여러 가지 블록을 조립해서 새로운 것을 만들 수 있어요. LangChain도 비슷해요! \n",
      "\n",
      "AI와 대화하기, 문서 읽기, 정보 기억하기 같은 여러 가지 기능들을 마치 블록처럼 쌓아서 여러분이 원하는 AI 프로그램을 만들 수 있답니다.\n",
      "\n",
      "예를 들면:\n",
      "- AI한테 질문하고 답변 받기\n",
      "- AI가 긴 문서를 읽고 중요한 내용 찾아주기\n",
      "- AI가 여러분의 정보를 기억했다가 나중에 사용하기\n",
      "\n",
      "이렇게 여러 가지 재미있는 일들을 할 수 있어요!\n"
     ]
    }
   ],
   "source": [
    "response = llm_claude3.invoke(message)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수학 선생님"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81을 9로 나누면 9입니다. (81 ÷ 9 = 9)\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"당신은  수학 문제를 잘 풀어주는 도움되는 assistant 입니다.\"),\n",
    "    HumanMessage(content=\"81을 9로 나누면 몇인가요?\")\n",
    "]\n",
    "\n",
    "result = llm_gpt4.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 곱하기 5는 50입니다.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"당신은  수학 문제를 잘 풀어주는 도움되는 assistant 입니다.\"),\n",
    "    HumanMessage(content=\"81을 9로 나누면 몇인가요?\"),\n",
    "    AIMessage(content=\"81을 9로 나누면 9입니다.\"),\n",
    "    HumanMessage(content=\"10 곱하기 5는 얼마인가요?\")\n",
    "]\n",
    "\n",
    "result = llm_gpt4.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model을 이용한 Conversation 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "사용자:  안녕. 내 이름은 오영제야\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: 안녕하세요, 오영제님! 만나서 반갑습니다. 어떻게 도와드릴까요?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "사용자:  내 이름이 뭐라고?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: 오영제님이시죠? 맞나요?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "사용자:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대화 History:\n",
      "[SystemMessage(content='You are a helpful AI assistant.'), HumanMessage(content='안녕. 내 이름은 오영제야'), AIMessage(content='안녕하세요, 오영제님! 만나서 반갑습니다. 어떻게 도와드릴까요?'), HumanMessage(content='내 이름이 뭐라고?'), AIMessage(content='오영제님이시죠? 맞나요?')]\n"
     ]
    }
   ],
   "source": [
    "# 채팅 기록을 저장할 리스트를 초기화\n",
    "chat_history = []\n",
    "\n",
    "# 시스템 메시지를 생성하여 채팅 기록에 추가\n",
    "system_message = SystemMessage(content=\"You are a helpful AI assistant.\")\n",
    "chat_history.append(system_message)\n",
    "\n",
    "# 무한 루프 시작\n",
    "while True:\n",
    "    # 사용자로부터 입력을 받음\n",
    "    query = input(\"사용자: \")\n",
    "    \n",
    "    # 사용자가 \"quit\"를 입력하면 루프를 종료\n",
    "    if query.lower() == \"quit\":\n",
    "        break\n",
    "    \n",
    "    # 사용자 메시지를 채팅 기록에 추가\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "\n",
    "    # llm_gpt4 모델을 사용하여 응답을 생성\n",
    "    result = llm_gpt4.invoke(chat_history)\n",
    "    response = result.content\n",
    "    \n",
    "    # AI 응답을 채팅 기록에 추가\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "    \n",
    "    # AI 응답을 출력\n",
    "    print(f\"AI: {response}\")\n",
    "\n",
    "# 전체 대화 기록을 출력\n",
    "print(\"대화 History:\")\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fi6_NvW3MaE"
   },
   "source": [
    "##  1. 프롬프트 템플릿과 로더를 사용하여 체인 구성  \n",
    "\n",
    "- LLM에서의 Chain 은 Data Processing 에서의 Pipeline 과 유사한 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G447ezl5X5ye"
   },
   "source": [
    "### 체인의 구성 요소 - Runnables\n",
    "- 프롬프트 : LLM의 응답을 안내하는 템플릿  \n",
    "- LLM 또는 채팅 모델 : 프롬프트에 따라 응답을 생성하는 엔진  \n",
    "- 출력 파서 : LLM의 출력을 파싱하는 도구  \n",
    "- 도구 : LLM이 API에서 추가 정보를 추출하거나 코드를 실행하여 LLM을 에이전트로 전환할 수 있게 해주는 확장 기능  \n",
    "- 일반 함수 : 서로 연결될 수 있는 추가적인 일반 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Yt1j9kXR1h53"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# simple prompt template 생성\n",
    "# {topic} 변수에서 사용자의 query 가 대체된다.\n",
    "prompt_template = \"\"\"\n",
    "    당신은 AI 주제를 설명하는 데 도움이 되는 어시스턴트입니다. 다음 입력이 주어지면:\n",
    "    {주제}\n",
    "    주어진 주제에 대한 설명을 제공하세요.\n",
    "\"\"\"\n",
    "\n",
    "# prompt template 을 이용하여 prompt 생성\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"주제\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "0vikrfE53yDk",
    "outputId": "adf40012-9025-4a1a-a591-da9fd5b4883c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 자연어 처리(NLP)와 관련된 다양한 작업을 수행하기 위해 설계된 프레임워크입니다. 이 프레임워크는 언어 모델을 활용하여 텍스트 생성, 질문 응답, 대화형 인터페이스 구축 등의 작업을 쉽게 구현할 수 있도록 도와줍니다. LangChain은 다양한 언어 모델과 상호작용할 수 있는 구조를 제공하며, 여러 데이터 소스와 통합하여 복잡한 언어 처리 작업을 수행할 수 있는 기능을 갖추고 있습니다.\n",
      "\n",
      "LangChain의 주요 특징은 다음과 같습니다:\n",
      "\n",
      "1. **모듈화된 설계**: LangChain은 다양한 구성 요소로 나뉘어 있어 사용자가 필요에 따라 모듈을 선택하고 조합하여 사용할 수 있습니다.\n",
      "\n",
      "2. **데이터 소스 통합**: LangChain은 데이터베이스, API, 파일 시스템 등 다양한 데이터 소스와 연결하여 정보를 검색하고 활용할 수 있습니다.\n",
      "\n",
      "3. **다양한 언어 모델 지원**: OpenAI의 GPT, Hugging Face의 Transformers 등 여러 언어 모델을 지원하여 사용자가 원하는 모델을 쉽게 사용할 수 있습니다.\n",
      "\n",
      "4. **유연한 워크플로우**: LangChain은 복잡한 언어 처리 작업을 위한 다양한 워크플로우를 제공하여 개발자들이 손쉽게 작업을 구성하고 실행할 수 있게 합니다.\n",
      "\n",
      "5. **대화형 애플리케이션 구축**: LangChain을 사용하면 대화형 챗봇이나 비서 애플리케이션을 쉽게 구축할 수 있습니다.\n",
      "\n",
      "이러한 특징들 덕분에 LangChain은 연구자, 개발자, 기업 등 다양한 분야에서 언어 모델을 활용한 프로젝트를 빠르고 효율적으로 진행할 수 있는 유용한 도구로 자리 잡고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# pipe operator \"|\"\" 를 이용하여 하나 이상의 chain 을 결합\n",
    "chain = prompt | llm_gpt4\n",
    "\n",
    "print(chain.invoke({\"주제\": \"LangChain이 뭐예요?\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJzXNZCnZRCr"
   },
   "source": [
    "### document loader 를 이용하여 script 요약\n",
    "더 고급 체인을 만들기 위해 LangChain을 사용하여 YouTube 비디오를 필사합니다. 먼저 YouTube Transcript API를 설치합니다. 그런 다음 LangChain의 커뮤니티 문서 로더에서 YouTube Loader를 가져옵니다. 로더에 비디오 URL을 전달하면 필사본을 추출할 수 있으며, 이는 문서 목록으로 반환됩니다. 원시 필사본을 얻으려면 이러한 문서에서 페이지 콘텐츠를 추출하거나 문서를 체인에 직접 전달하기만 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "L1HMGE6k4mQp"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3r04ZHou46-F"
   },
   "outputs": [],
   "source": [
    "# LangChain 커뮤니티에서 제공하는 Youtube Loader를 임포트\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "# YoutubeLoader를 사용하여 유튜브 URL에서 로더를 생성\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=AOEGOhkGtjI\", add_video_info=False\n",
    ")\n",
    "\n",
    "# 비디오의 자막을 문서로 로드\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "2Nh1VYeQ5tdt",
    "outputId": "ea0d5e86-a0bd-49f7-b7f4-0721bfedac22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama 3 and some of the capabilities of the model you can see how the model compares to other popular llms specifically the 70b model that I'm going to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain all right the first thing I'm going to do is I'm going to set up the cop notebook and pip install the required libraries then I'll connect to Bay and fetch the schema information from tables in a data set I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions that extracts the schema information from bit query in a schema. py file and this is the same functions that I've been using in the earlier videos that lets me extract and feed the scheme information from bitre to the chain all right so I'm just going to load the environment variables and then I'm going to connect to biy and the data set I'm using is the same data set I used in the last video in the dashboard video it is an e-commerce data set with four tables customers orders products and customer taxs and the two functions in the schema. py file allows me to extract the schema information from the data set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this case I'm using Lama 370b and the prom template will be injecting three things first I'll inject the schema information I'm extracting from the bit crate data set then I'm injecting the main question or the query and finally I'll be injecting a a message history and the message history is The Tweak I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch the error and feed it back to the chain and this allows me to make the chains self-correcting which is useful when we're dealing with a model that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to to inject the schema information and to inject the messages of the message history that contains the errors then I use my prompt the language model and a string output passer and this is what we need to generate the SQL code from a prompt now let's move on to generate some insights with this setup I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets me extract the SQL code from the response and in this extract SQL function I'm simply using regex to extract the SQL from whatever the language model is returning to wrap it all up I'm creating a function that takes a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error I'm going to collect the error and feed it back to the chain and try again and in this way the chain will be self-corrected ing because the llm will understand the error message okay so let's try this the first prompt I'm going to give L three is the following give me a list of the best customers including their rank their first name last name and email and the products they purchased and this one it got in the first attempt so the query executed successfully and we can then have a look at the data frame and this is essentially an audience that you could use for marketing purposes you normally create an audience like this in a customer data platform now let's try a different one I'll do a classical one show me the revenue generated in the last 30 days broken down by acquisition Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the error message makes the second attempt successful and here we have Revenue broken down by acquisition source let's do another audience let's say I want the top 100 customers with the highest purchase frequency but with an under average aov and again we see that the first attempt fails and the second attempt is successful and here we only got the names let's say that we want to include the frequency and the aov as well to get the full overview and here we see that the first attempt fails the second attempt also fails but the third attempt is a success and here we have the full audience data frame with the purchase frequency and the average order value so this is very useful so we can use llama 3 for generating insights if we just implement this small tweak of catching the errors and feeding it back to the chain all right so what are the implications of this first of all we now know that with Lama 3 open source llms can be used to generate insights and this is very good news for privacy sensitive use cases so use cases where you want to feed sensitive customer data back to the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use Lama 3 locally this is also very good news for query heavy applications so text tosql applications can be query heavy if they're rolled out in a big organization so there's a cost consideration that might be worth looking into now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really useful for Consumer facing applications where speed is necessary finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm generated in the future and not so distant future so if you are a data analyst or data engineer you should really pay attention to this and learn this new technology all right that's it for now if you enjoyed this video I suggest you check out one of the other videos on generating SQL with llm chains thanks for watching\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = docs[0].page_content\n",
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEScwvtSck_C"
   },
   "source": [
    "이제, 대본이 주어진 YouTube 비디오를 요약하는 체인을 설정해 보겠습니다. 비디오 대본을 입력 변수로 프롬프트 템플릿에 주입한 다음 파이프 연산자를 사용하여 체인을 설정합니다. 비디오 대본으로 호출하면 원시 텍스트를 수동으로 추출하지 않고도 간결한 요약을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyEU9cQo7kZF",
    "outputId": "40c8d37c-dffe-40f2-caab-0d0348e7e0e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['video_transcript'], template='\\n    당신은 유튜브 비디오를 요약하는 유용한 조수입니다. 다음 비디오 대본이 주어질 때:\\n    {video_transcript}\\n    한국어로 요약해 주세요.\\n')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전사된 내용을 chain에 사용합니다.\n",
    "# prompt template 생성\n",
    "prompt_template = \"\"\"\n",
    "    당신은 유튜브 비디오를 요약하는 유용한 조수입니다. 다음 비디오 대본이 주어질 때:\n",
    "    {video_transcript}\n",
    "    한국어로 요약해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "# prompt instance 생성\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"video_transcript\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MXvV726M8VPN"
   },
   "outputs": [],
   "source": [
    "# chain 생성\n",
    "chain = prompt | llm_gpt4\n",
    "\n",
    "answer = chain.invoke({\"video_transcript\": docs}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OW1qGhFF8e3f",
    "outputId": "d64990be-176c-4715-e60b-4331a8f208bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 비디오는 Meta의 Llama 3 모델을 활용하여 데이터 분석을 최적화하는 방법에 대해 다룹니다. Llama 3는 텍스트를 SQL로 변환하는 데 빠르고 고급 SQL을 생성할 수 있는 능력을 가지고 있습니다. 비디오에서는 SQL 체인을 최적화하는 방법과 Llama 3의 통찰력을 추출하는 방법에 대해 설명합니다.\n",
      "\n",
      "비디오의 주요 내용은 다음과 같습니다:\n",
      "\n",
      "1. **Llama 3 모델 소개**: Llama 3는 데이터 분석에 있어 게임 체인저가 될 것으로 기대됩니다.\n",
      "2. **SQL 체인 설정**: Google Cloud에서 Llama 3를 사용하여 SQL 체인을 설정하는 과정이 설명됩니다.\n",
      "3. **오류 처리 및 자가 수정**: SQL 코드 생성 과정에서 발생하는 오류를 처리하고, 이를 통해 모델이 자가 수정할 수 있도록 하는 방법이 소개됩니다.\n",
      "4. **데이터 분석 사례**: 다양한 SQL 쿼리 예시를 통해 Llama 3가 어떻게 유용한 인사이트를 생성하는지 보여줍니다. 예를 들어, 고객 목록, 최근 30일의 수익, 구매 빈도 등을 분석합니다.\n",
      "5. **미래의 데이터 분석**: Llama 3와 같은 오픈 소스 LLM이 데이터 분석에 활용될 것이며, 데이터 분석가와 엔지니어는 이러한 기술을 배우고 활용해야 한다는 점이 강조됩니다.\n",
      "\n",
      "결론적으로, Llama 3는 개인 정보 보호가 중요한 경우와 쿼리가 많은 애플리케이션에서 특히 유용할 것이며, 향후 데이터 파이프라인과 보고서는 LLM에 의해 생성될 가능성이 높습니다.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWWyBVnI82BW"
   },
   "source": [
    "### Built-in Chains 이용 예\n",
    "\n",
    "- create_stuff_documents_chain : 모델에 문서 목록을 전달하기 위한 체인을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QG0H9E858fh0"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# create_stuff_documents_chain은 문서 목록을 받아 모두 하나의 프롬프트로 형식화합니다.\n",
    "# create_stuff_documents_chain 의 input variable 이름은 context 여야 합니다.\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    당신은 유튜브 비디오를 요약하는 유용한 조수입니다. 다음 비디오 대본이 주어질 때:\n",
    "    {context}\n",
    "    한국어로 요약해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "# prompt instance 생성\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# chain 생성\n",
    "chain = create_stuff_documents_chain(llm_gpt4, prompt)\n",
    "\n",
    "answer = chain.invoke({\"context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "uAuC8u_--akr",
    "outputId": "ac21eafa-1390-4c7d-c289-3ebabdf1f2d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 비디오에서는 메타의 Lama 3 모델을 사용하여 데이터 분석을 위한 SQL 체인을 최적화하는 방법에 대해 설명합니다. 이 모델은 텍스트를 SQL로 변환하는 데 있어 빠르고 고급 SQL 생성을 가능하게 하며, 이를 통해 데이터 분석에서 큰 변화를 가져올 것으로 기대됩니다.\n",
      "\n",
      "비디오의 주요 내용은 다음과 같습니다:\n",
      "\n",
      "1. **Lama 3 소개**: Lama 3 모델은 데이터 분석에 있어 혁신적인 도구로, 다른 인기 있는 LLM 모델들과 비교할 수 있는 성능을 보여줍니다.\n",
      "\n",
      "2. **설정 과정**: Gro Cloud를 사용하여 Lama 3에 연결하고 SQL 체인을 설정하는 과정이 설명됩니다. 필요한 라이브러리를 설치하고, 데이터셋의 스키마 정보를 가져옵니다.\n",
      "\n",
      "3. **SQL 코드 생성**: Lama 3를 이용해 SQL 코드를 생성하고, 오류가 발생할 경우 이를 자동으로 수정하여 재시도하는 방법이 다루어집니다.\n",
      "\n",
      "4. **인사이트 생성**: 다양한 SQL 쿼리를 실행하여 고객 데이터, 수익 분석 등의 인사이트를 추출하는 과정이 보여집니다. 첫 번째 시도가 실패하더라도 오류를 피드백하여 성공적인 결과를 도출하는 방식이 강조됩니다.\n",
      "\n",
      "5. **미래 전망**: Lama 3를 통해 오픈 소스 LLM이 인사이트 생성에 활용될 수 있으며, 이는 개인정보 보호에 민감한 사례에서 특히 유용합니다. 데이터 분석가나 엔지니어는 이러한 기술을 배우고 활용해야 할 필요성이 강조됩니다.\n",
      "\n",
      "비디오를 마치며, SQL 생성과 관련된 다른 비디오를 추천합니다.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Runnable과 LangChain 표현 언어  \n",
    "\n",
    "- LCEL(LangChain Expression Language) 는 Runnables 를 chain 으로 구성하는 방법\n",
    "\n",
    "LCEL은 기본 구성 요소에서 복잡한 체인을 구축하는 것을 간소화합니다. 파이프 연산자(|)를 사용하여 다양한 구성 요소를 체인으로 연결하고 한 요소에서 다음 요소로 출력을 공급합니다. 이런 방식으로 구성된 체인의 간단한 예로는 모델과 출력 파서가 결합된 프롬프트가 있습니다.  이러한 구성 요소들을 runnables 라고 부릅니다.  \n",
    "\n",
    "`chain=prompt | model | output_parser`\n",
    "\n",
    "- Chain 구성  \n",
    "    - 체인에 대한 입력(일반적으로 사전)\n",
    "    - 입력은 프롬프트로 전송됩니다.\n",
    "    - 프롬프트 값은 LLM 또는 채팅 모델로 전송됩니다.\n",
    "    - Chatmodel이 채팅 메시지를 반환합니다.\n",
    "    - 파서는 채팅 메시지에서 문자열을 추출합니다.\n",
    "    - 문자열은 체인의 출력입니다\n",
    "\n",
    "- LangChain의 runnable 객체들:\n",
    "\n",
    "    - RunnableSequence : 여러 runnable 구성 요소를 연결하여 각 구성 요소가 입력을 처리하고 출력을 다음 구성 요소에 전달\n",
    "    - RunnableLambda : Python의 호출 가능한 요소(함수 등)를 실행 가능한 구성 요소로 바꿔서 체인으로 통합\n",
    "    - RunnablePassthrough : 입력을 변경하지 않고 통과시키거나 출력에 추가 키를 추가. placeholder 역할을 하거나 시퀀스에 유연하게 통합할 수 있다.\n",
    "    - RunnableParallel : 여러 개의 실행 파일을 동시에 실행하여 두 개의 체인이 동일한 입력에서 실행되지만 다른 출력을 반환하는 분기를 허용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "3s0DaSYO_W0Y"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "summarize_prompt_template = \"\"\"\n",
    "    당신은 AI 개념을 요약하는 유용한 조수입니다.\n",
    "    {context}\n",
    "    context를 한국어로 요약해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(summarize_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InN3aD1qGwmD"
   },
   "source": [
    "###  \"|\" 연산자를 이용하여 Runnable Sequence chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0cahXhesl-E",
    "outputId": "eed661f4-9190-4da5-f8df-0139d0df7b64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "WpJAZmpOsjHA",
    "outputId": "147f9a53-6d84-4d3c-cae6-4322635e23f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain은 언어 모델과 관련된 다양한 작업을 쉽게 수행할 수 있도록 돕는 프레임워크입니다. 이 프레임워크는 언어 모델을 활용하여 정보 검색, 대화 생성, 데이터 처리 등 여러 기능을 통합할 수 있게 해줍니다. LangChain은 다양한 API와 도구를 연결하여 사용자가 원하는 기능을 손쉽게 구현할 수 있도록 설계되었습니다. 주로 개발자들이 언어 모델을 활용한 애플리케이션을 개발할 때 유용하게 사용됩니다. \\n\\n요약하자면, LangChain은 언어 모델을 효과적으로 활용하기 위한 프레임워크입니다.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_VLKGZ1HO7d"
   },
   "source": [
    "### RunnableLambda 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oI9MZ0sasxQf",
    "outputId": "c5356d5c-6811-43af-9bfb-31fe88e2638e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'langchain_core.runnables.base.RunnableLambda'>\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# RunnableLambda를 사용하여 Python 함수를 체인에 삽입합니다.\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "print(type(summarize_chain))\n",
    "\n",
    "# 사용자 정의 람다 함수를 정의하고 이를 RunnableLambda에 래핑합니다.\n",
    "length_lambda = RunnableLambda(lambda summary: f\"요약된 길이: {len(summary)} 글자\")\n",
    "print(type(length_lambda))\n",
    "\n",
    "lambda_chain = summarize_chain | length_lambda\n",
    "print(type(lambda_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TAa1W6iVHLnq",
    "outputId": "5b181616-a4f7-49a1-b807-9795fc389b9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'요약된 길이: 251 글자'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhIbYeOgIYyi",
    "outputId": "6ee972ca-751b-4b86-a8cd-fefe6fbe8c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context'] template='\\n    당신은 AI 개념을 요약하는 유용한 조수입니다.\\n    {context}\\n    context를 한국어로 요약해 주세요.\\n'\n",
      "client=<openai.resources.chat.completions.Completions object at 0x000002A2BC85AB50> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002A2BDF584F0> model_name='gpt-4o-mini' openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "\n",
      "RunnableLambda(lambda summary: f'요약된 길이: {len(summary)} 글자')\n"
     ]
    }
   ],
   "source": [
    "for step in lambda_chain.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8q2V33ttqTg"
   },
   "source": [
    "함수를 명시적으로 RunnableLambda로 변환하지 않고도 체인에서 함수를 사용할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydx2GKyENIna",
    "outputId": "b53816bc-77c8-4e11-de5f-9362b79bc6ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda summary: f'요약된 길이: {len(summary)} 글자')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RunnableLambda로 변환하지 않고 체인에서 함수 사용\n",
    "chain_without_function = summarize_chain | (lambda summary: f\"요약된 길이: {len(summary)} 글자\")\n",
    "\n",
    "chain_without_function.steps[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "00FqsywiNyq6",
    "outputId": "019c1f51-59e1-4b93-fc51-85d18097db15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'요약된 길이: 527 글자'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_without_function.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktwM5hWrOTvQ"
   },
   "source": [
    "### placeholder 로 사용되는 RunnablePassthrough\n",
    "\n",
    "체인 내에서 데이터를 변경하지 않고 다음 단계로 전달하는 데 사용됩니다. 주로 디버깅하거나 체인의 특정 부분을 테스트할 때 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cnTjJX9OOOPj",
    "outputId": "72c6fc6b-540c-4ad4-ae24-b473f9941c09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'요약된 길이: 291 글자'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "\n",
    "# RunnablePassthrough instance 생성\n",
    "passthrough = RunnablePassthrough()\n",
    "\n",
    "# 요약 및 길이 계산과 함께 파이프 연산자를 사용하여 시퀀스 생성\n",
    "placeholder_chain = summarize_chain | passthrough | length_lambda\n",
    "\n",
    "placeholder_chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZCrH_wbPRrw",
    "outputId": "4ee15bed-aafd-45d7-e976-785d274c6a5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptTemplate(input_variables=['context'], template='\\n    당신은 AI 개념을 요약하는 유용한 조수입니다.\\n    {context}\\n    context를 한국어로 요약해 주세요.\\n'),\n",
       " ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002A2BC85AB50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002A2BDF584F0>, model_name='gpt-4o-mini', openai_api_key=SecretStr('**********'), openai_proxy=''),\n",
       " StrOutputParser(),\n",
       " RunnablePassthrough(),\n",
       " RunnableLambda(lambda summary: f'요약된 길이: {len(summary)} 글자')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_chain.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOnoJI94Pu0Z"
   },
   "source": [
    "### RunnablePassthrough for assignment\n",
    "- 데이터를 그대로 통과시키는 역할을 하면서 추가적으로 특정 키에 값을 할당\n",
    "- 체인을 통과하는 데이터에 \"요약\" 필드의 길이를 계산하고 length라는 키로 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fb-G8U7CPY_p",
    "outputId": "14a58845-690c-4989-96b5-9e796e6c1789"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'요약': 'LangChain은 자연어 처리(NLP) 애플리케이션을 구축하기 위한 프레임워크입니다. 이 프레임워크는 언어 모델과 다른 컴포넌트를 통합하여 복잡한 작업을 수행할 수 있도록 지원합니다. LangChain은 다양한 데이터 소스와 상호작용하고, 텍스트 기반의 작업을 처리하며, 사용자와의 상호작용을 개선하는 데 도움을 주는 도구와 모듈을 제공합니다. 이를 통해 개발자들은 보다 효율적으로 맞춤형 NLP 솔루션을 만들 수 있습니다.',\n",
       " 'length': 238}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약을 dictionary로 변환하는 사용자 정의 람다 함수 정의\n",
    "wrap_summary_lambda = RunnableLambda(lambda summary: {\"요약\": summary})\n",
    "\n",
    "# Dict 입력에 mapping argument를 merge RunnablePassthrough 인스턴스 생성\n",
    "assign_passthrough = RunnablePassthrough.assign(length=lambda x: len(x['요약']))\n",
    "\n",
    "# summarization chain 생성\n",
    "summarize_chain = summarize_prompt | llm_gpt4 | output_parser | wrap_summary_lambda\n",
    "\n",
    "# 요약과 assign_passthrough를 결합하여 전체 체인을 생성\n",
    "assign_chain = summarize_chain | assign_passthrough\n",
    "\n",
    "# Use the chain\n",
    "result = assign_chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfAvpNwmxGga",
    "outputId": "f0bb779e-40d0-4369-a8e1-31c33029a6df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.passthrough.RunnableAssign'>\n"
     ]
    }
   ],
   "source": [
    "print(type(assign_chain.steps[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87QUb996RXRC"
   },
   "source": [
    "### RunnableParallel \n",
    "- 체인의 특정 단계에서 입력을 동시에 여러 작업에 전달하고 각각의 결과를 수집하여 반환합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flTnjKw1QykS",
    "outputId": "9dd1775f-1a43-4ae2-beaf-11257781a9d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'요약': 'LangChain은 자연어 처리(NLP) 응용 프로그램을 구축하기 위한 프레임워크입니다. 이 프레임워크는 언어 모델(예: GPT-3, GPT-4 등)을 활용하여 다양한 작업을 수행할 수 있도록 돕습니다. LangChain은 데이터 소스와의 통합, 대화형 응용 프로그램 개발, 그리고 모델의 효율적인 사용을 위한 다양한 구성 요소와 도구를 제공합니다. 이를 통해 개발자들은 더 쉽고 효율적으로 강력한 언어 기반 시스템을 구축할 수 있습니다. LangChain은 특히 대화형 AI, 챗봇, 정보 검색 등의 분야에서 유용하게 사용됩니다.'},\n",
       " 'length': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "#summarization chain 생성\n",
    "summarization_chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "\n",
    "# summary 와 길이 계산을 병렬로 처리하기 위해 RunnableParallel 인스턴스를 만듭니다.\n",
    "parallel_runnable = RunnableParallel(\n",
    "    summary=lambda x: x,  # 현재의 summary 전달\n",
    "    length=lambda x: len(x)   # summary의 길이 계산\n",
    ")\n",
    "\n",
    "# parellel runnable과 summarization 결합\n",
    "parallel_chain = summarize_chain | parallel_runnable\n",
    "\n",
    "parallel_chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2s-x-iRvWrY"
   },
   "source": [
    "## 3. RAG Chain 만들기\n",
    "\n",
    "RAG는 외부 데이터로 대형 언어 모델(LLM)의 지식을 증강하는 기술입니다. RAG 애플리케이션은 두 단계로 구축할 수 있습니다:   \n",
    "   \n",
    "1) 외부 데이터 색인화   \n",
    "2) 검색 및 출력 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./db/chroma_db_uracle'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "# directory 경로 정의\n",
    "persistent_directory = \"./db/chroma_db_uracle\"\n",
    "persistent_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document Chunks Information ---\n",
      "Number of document chunks: 5\n",
      "Sample chunk:\n",
      "유라클: 국내 1위 모바일 플랫폼 소프트웨어 – 모바일 소프트웨어, 모피어스와 블록체인 미들웨어 플랫폼. 헤카테를 보유한 기업입니다.\n",
      "\n",
      "Skip to content\n",
      "\n",
      "\n",
      "유라클: 국내 1위 모바일 플랫폼 소프트웨어\n",
      "모바일 소프트웨어, 모피어스와 블록체인 미들웨어 플랫폼. 헤카테를 보유한 기업입니다.\n",
      "\n",
      "Business \n",
      "\n",
      "Morpheus Suite\n",
      "\n",
      "하이브리드 앱\n",
      "콘텐츠\n",
      "웹 앱\n",
      "쿠폰\n",
      "푸시\n",
      "서베이\n",
      "메시지\n",
      "\n",
      "\n",
      "Athena Suite\n",
      "\n",
      "sLLM\n",
      "LLMOps\n",
      "\n",
      "\n",
      "Customer \n",
      "\n",
      "Customer유라클의 1,000여 기업 고객사례를 소개합니다.\n",
      "레퍼런스\n",
      "고객 사례\n",
      "기술지원\n",
      "문의하기\n",
      "\n",
      "\n",
      "Partner \n",
      "\n",
      "Partner국내 및 글로벌 사업을 함께하는 파트너 프로그램\n",
      "파트너 프로그램\n",
      "\n",
      "\n",
      " Company \n",
      "\n",
      "Company 대한민국 모바일, AI 플랫폼 리더유라클\n",
      "회사소개\n",
      "연혁\n",
      "CI/BI\n",
      "수상/인증\n",
      "인재채용\n",
      "지속가능경영\n",
      "\n",
      "\n",
      "PR/IR \n",
      "\n",
      "PR/IR대한민국 모바일, AI 플랫폼 리더유라클의 소식을 소개합니다.\n",
      "IR/공지\n",
      "보도자료\n",
      "뉴스레터\n",
      "\n",
      "\n",
      "Blog\n",
      "\n",
      " \n",
      "BusinessMorpheus Suite하이브리드 앱 콘텐츠 웹 앱 쿠폰 푸시 서베이 메시지  Athena SuitesLLM LLMOps   CustomerCustomer유라클의 1,000여 기업 고객사례를 소개합니다. 레퍼런스 고객 사례 기술지원 문의하기  PartnerPartner국내 및 글로벌 사업을 함께하는 파트너 프로그램 파트너 프로그램  CompanyCompany 대한민국 모바일, AI 플랫폼 리더유라클 회사소개 연혁 CI/BI 수상/인증 인재채용 지속가능경영  PR/IRPR/IR대한민국 모바일, AI 플랫폼 리더유라클의 소식을 소개합니다. IR/공지 보도자료 뉴스레터  Blog\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: WebBaseLoader를 사용하여 apple.com에서 콘텐츠 스크래핑\n",
    "# WebBaseLoader는 웹 페이지를 로드하고 해당 내용을 추출합니다.\n",
    "urls = [\"https://uracle.co.kr/\"]\n",
    "\n",
    "# 웹 콘텐츠에 대한 로더를 만듭니다\n",
    "loader = WebBaseLoader(urls)\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: 스크래핑된 콘텐츠를 청크로 분할\n",
    "# CharacterTextSplitter는 텍스트를 더 작은 덩어리로 나눕니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 분할된 문서에 대한 정보 표시\n",
    "print(\"\\n--- Document Chunks Information ---\")\n",
    "print(f\"Number of document chunks: {len(docs)}\")\n",
    "print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ./db/chroma_db_uracle에 벡터 저장소 생성 ---\n",
      "--- ./db/chroma_db_uracle에 벡터 저장소 생성 완료 ---\n"
     ]
    }
   ],
   "source": [
    "# Step 3: 문서 청크에 대한 임베딩을 만듭니다.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Step 4: 임베딩을 사용하여 벡터 저장소를 생성하고 유지합니다.\n",
    "if not os.path.exists(persistent_directory):\n",
    "    print(f\"\\n--- {persistent_directory}에 벡터 저장소 생성 ---\")\n",
    "    db = Chroma.from_documents(\n",
    "        docs, embeddings, persist_directory=persistent_directory)\n",
    "    print(f\"--- {persistent_directory}에 벡터 저장소 생성 완료 ---\")\n",
    "else:\n",
    "    print(f\"벡터 저장소 {persistent_directory}가 이미 존재 합니다. 새로이 생성하지 않았습니다.\")\n",
    "    db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)\n",
    "\n",
    "# Step 5: 벡터 저장소 쿼리\n",
    "# 벡터 저장소를 쿼리하기 위한 검색기 생성\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "JgOnOVNfvAnO"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"\n",
    "    다음 내용에만 근거하여 질문에 답하세요:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "AwMZCi70AmY_"
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    # \"context\"는 입력 데이터에서 'question' 값을 가져오고, 이를 retriever에 전달하여 유사한 문서 검색\n",
    "    {\"context\": (lambda x: x['question']) | retriever,\n",
    "     \"question\": (lambda x: x['question'])}  # \"question\"은 입력 데이터에서 'question' 값을 그대로 반환\n",
    "    | prompt    # 검색된 문서를 이용해 프롬프트를 생성함\n",
    "    | llm_gpt4   # 프롬프트를 기반으로 GPT-4 모델을 사용해 응답을 생성\n",
    "    | StrOutputParser()    # LLM의 출력을 문자열로 파싱\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k56mgJ9Ix_Gt",
    "outputId": "6c6d5ace-7582-4d24-8d86-9f63940b27b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유라클은 모바일 플랫폼 소프트웨어를 전문으로 하는 회사로, 하이브리드 앱, 웹 앱, 푸시 알림, UMS(통합 메시징 서비스), 콘텐츠 관리, 쿠폰, 서베이 등의 서비스를 제공합니다. 또한, AI 기반 메시지 서비스인 'Message.AI'를 출시하고 모바일 헬스케어 사업을 확대하는 등 AI 시장에도 진출하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "answer = chain.invoke({\"question\": \"Uracle 이 주로 하는 일이 뭐야?\"})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 체인이 달린 도구 사용  \n",
    "\n",
    "- 도구는 에이전트, 체인 또는 대형 언어 모델(LLM)이 세상과 상호 작용할 수 있게 하는 인터페이스입니다.\n",
    "    - Python REPL, Wikipdedia, YouTube, Zapier, Gradio, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "RkuqKhVpyKxm"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet youtube_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZF5O2LeB-vD",
    "outputId": "86cbdc7a-8466-410b-864b-7c67acc519db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=-ZRP1nnjRSo&pp=ygUP7L2U65Sp7ZWY64qUIEFJ', 'https://www.youtube.com/watch?v=O5-cNXWWdzs&pp=ygUP7L2U65Sp7ZWY64qUIEFJ']\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "# YouTubeSearchTool 인스턴스 생성\n",
    "youtube_tool = YouTubeSearchTool()\n",
    "\n",
    "# 아무 키워드로 유튜브 검색 실행\n",
    "results = youtube_tool.run(\"코딩하는 AI\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5BZ28LTAtul",
    "outputId": "62f913a7-6b6c-415c-d494-4ad127a6a18d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xz3xnc9nemDc4SeKY2jsXXUW', 'function': {'arguments': '{\"query\":\"코딩하는 AI\"}', 'name': 'youtube_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 94, 'total_tokens': 111, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0ba0d124f1', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-fd10381c-e173-4768-b886-4189c29895b0-0', tool_calls=[{'name': 'youtube_search', 'args': {'query': '코딩하는 AI'}, 'id': 'call_xz3xnc9nemDc4SeKY2jsXXUW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 94, 'output_tokens': 17, 'total_tokens': 111})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YouTube 도구를 LLM에 바인딩하여 parameter 정보를 알아냄\n",
    "llm_with_tools = llm_gpt4.bind_tools([youtube_tool])\n",
    "\n",
    "msg = llm_with_tools.invoke(\"코딩하는 AI\")\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHYOhC0BCPPe",
    "outputId": "7dd07d3c-7316-4d98-849d-5e9e025e67ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'youtube_search',\n",
       "  'args': {'query': '코딩하는 AI'},\n",
       "  'id': 'call_xz3xnc9nemDc4SeKY2jsXXUW',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_with_tools 에서 parameter 정보를 알아낸다\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '코딩하는 AI'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.tool_calls[0][\"args\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-D_-T27Azsy",
    "outputId": "87aec236-a536-48f7-8af1-bbbc8fd8f656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002A2BC85AB50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002A2BDF584F0>, model_name='gpt-4o-mini', openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'youtube_search', 'description': 'search for youtube videos associated with a person. the input to this tool should be a comma separated list, the first part contains a person name and the second a number that is the maximum number of video results to return aka num_results. the second part is optional', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string'}}, 'required': ['query']}}}]})\n",
       "| RunnableLambda(lambda x: x.tool_calls[0]['args'])\n",
       "| YouTubeSearchTool()"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_with_tools에서 추출된 인수로 \n",
    "# YouTubeSearchTool을 사용하여 유튜브 검색 실행한는 chain 생성\n",
    "chain = llm_with_tools | (lambda x: x.tool_calls[0][\"args\"]) | youtube_tool\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"코딩하는 AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDYHTp3LIzr4",
    "outputId": "48b727f3-bd82-4602-ebdd-0a257fa76799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.youtube.com/watch?v=-ZRP1nnjRSo&pp=ygUP7L2U65Sp7ZWY64qUIEFJ',\n",
       " 'https://www.youtube.com/watch?v=O5-cNXWWdzs&pp=ygUP7L2U65Sp7ZWY64qUIEFJ']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "ast.literal_eval(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 : Prompt 수정하여 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
