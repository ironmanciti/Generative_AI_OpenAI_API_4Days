{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TypDOiTPbjE"
   },
   "source": [
    "# LangChain 소개\n",
    "\n",
    "- 프롬프트 템플릿과 로더를 사용하여 체인 구성  \n",
    "- 실행 가능한 프로토콜과 LangChain 표현 언어  \n",
    "- 벡터스토어 지원 리트리버 생성  \n",
    "- RAG 체인 구축  \n",
    "- 체인이 달린 도구 사용  \n",
    "- 도구에 액세스할 수 있는 에이전트 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOUiSLMxPzZY"
   },
   "source": [
    "## 1. LangChain 개요\n",
    "\n",
    "LangChain으로 개발된 애플리케이션은 일반적으로 세 가지 범주로 나뉩니다.\n",
    "\n",
    "- 챗봇 : LLM을 이용하여 보다 지능적인 마케팅, 고객 지원, 교육 상호작용 구현.\n",
    "- 검색 증강 생성(RAG) Q&A : 대용량 문서 요약, 데이터 분석, 외부 소스를 참조하여 코드 생성에 사용.\n",
    "- 에이전트 시스템은 다중 에이전트 설정과 인간 상호작용을 포함하며 복잡한 워크플로우를 위해 LangGraph를 활용. 공급망 관리 및 운영 최적화와 같은 분야에 적용 가능.\n",
    "\n",
    "LangChain은 자연어를 실행 가능한 프로그램으로 변환하는 파이프라인을 생성할 수 있게 합니다. 이러한 체인을 활용함으로써 사용자는 자연어를 입력하고 보고서, 분석 또는 컴퓨터 프로그램과 같은 출력을 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNdbWOsusEgx",
    "outputId": "c8df9de3-aabf-459f-bcfa-fe09c692bfbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "python-dotenv \\\n",
    "langchain \\\n",
    "langchain-community \\\n",
    "openai \\\n",
    "anthropic \\\n",
    "langchain-openai \\\n",
    "langchain-anthropic \\\n",
    "langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FXRwb1YcsW1j",
    "outputId": "7e5cff59-b6f5-446d-e3cc-93891014f5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtDVgGQiQ8eq"
   },
   "source": [
    "env 파일에서 API 키를 로드하고 OpenAI 및 Anthropic API에 연결합니다. 원하는 모델을 지정하여 ChatOpenAI 및 ChatAnthropic 클래스의 인스턴스를 만듭니다. llm_claude3 인스턴스를 사용하여 간단한 쿼리를 호출하여 설정을 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEi1gjmpW8ab"
   },
   "source": [
    "## LLM 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jSY2uUq8ssCp"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm_gpt4 = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\")\n",
    "\n",
    "llm_claude3 = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "NFWMvMRitGX-",
    "outputId": "9eff0799-8467-4610-ee63-d3b945657364"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain은 언어 모델을 활용하여 다양한 애플리케이션을 개발하기 위한 프레임워크입니다. 이 프레임워크는 언어 모델을 기반으로 한 작업을 쉽게 구축하고 관리할 수 있도록 도와줍니다. LangChain의 주요 기능은 다음과 같습니다:\\n\\n1. **모델 연결**: LangChain은 OpenAI, Hugging Face 등 다양한 언어 모델과 쉽게 연결할 수 있는 인터페이스를 제공합니다.\\n\\n2. **체인 구조**: LangChain은 작업을 체인 형태로 구성할 수 있도록 지원합니다. 이를 통해 여러 단계를 거치는 복잡한 작업을 간단하게 구현할 수 있습니다.\\n\\n3. **데이터 연결**: LangChain은 데이터베이스, API 등 다양한 데이터 소스와 통합할 수 있는 기능을 제공하여, 언어 모델이 더 많은 맥락을 이해하고 활용할 수 있도록 합니다.\\n\\n4. **응용 프로그램 개발**: LangChain을 사용하면 챗봇, 질문 답변 시스템, 정보 검색 시스템 등 다양한 언어 기반 애플리케이션을 쉽게 개발할 수 있습니다.\\n\\n5. **유연한 확장성**: 개발자는 LangChain을 기반으로 자신만의 맞춤형 기능을 추가하거나 기존 기능을 확장할 수 있습니다.\\n\\nLangChain은 이러한 기능을 통해 개발자들이 언어 모델을 활용하여 창의적이고 유용한 애플리케이션을 만들 수 있도록 지원합니다.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_gpt4.invoke(\"한국어로 LangChain 에 대해서 설명해줘\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "q9Y-0gAhxCx2",
    "outputId": "7c94fbd4-7612-4799-9ab7-6c84c7831fc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain은 자연어 처리와 언어 모델을 활용하여 다양한 애플리케이션을 구축할 수 있도록 도와주는 파이썬 라이브러리입니다. 이 라이브러리는 대화형 AI 시스템, 질의응답 시스템, 문서 요약 도구 등 다양한 자연어 처리 작업을 간편하게 수행할 수 있도록 설계되었습니다.\\n\\nLangChain의 주요 기능은 다음과 같습니다:\\n\\n1. 프롬프트 템플릿: 사용자는 프롬프트 템플릿을 정의하여 언어 모델에 입력할 내용을 쉽게 구성할 수 있습니다. 이를 통해 일관성 있고 구조화된 입력을 생성할 수 있습니다.\\n\\n2. 에이전트: LangChain은 에이전트 시스템을 제공하여 언어 모델과 외부 도구를 연결할 수 있습니다. 에이전트는 사용자의 질의를 분석하고, 필요한 정보를 수집하기 위해 외부 도구와 상호 작용합니다.\\n\\n3. 메모리: 대화형 시스템에서 이전 대화 내용을 기억하고 활용할 수 있도록 메모리 기능을 제공합니다. 이를 통해 문맥을 이해하고 이전 대화를 참조하여 더 나은 응답을 생성할 수 있습니다.\\n\\n4. 문서 로더: 다양한 형식의 문서(PDF, 웹 페이지, 워드 문서 등)를 로드하고 분석할 수 있는 기능을 제공합니다. 이를 통해 문서 기반의 질의응답 시스템을 구축할 수 있습니다.\\n\\n5. 벡터 스토어: 대규모 문서 집합에서 유사한 문서를 빠르게 검색할 수 있도록 벡터 스토어를 활용합니다. 이는 효율적인 정보 검색과 추천 시스템 구축에 유용합니다.\\n\\nLangChain은 OpenAI의 GPT 모델, Hugging Face의 Transformers 라이브러리 등 다양한 언어 모델과 호환되며, 사용자가 선호하는 모델을 선택하여 활용할 수 있습니다.\\n\\nLangChain을 사용하면 복잡한 자연어 처리 작업을 보다 쉽게 구현할 수 있으며, 대화형 AI 시스템, 챗봇, 지식 기반 질의응답 시스템 등 다양한 애플리케이션을 효과적으로 개발할 수 있습니다.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_claude3.invoke(\"한국어로 LangChain 에 대해서 설명해줘\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "aiKZpBOHnYJn",
    "outputId": "d2201765-56c3-49ef-a9de-3c45f9b0be74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## LangChain: 한국어 설명\\n\\nLangChain은 **대규모 언어 모델 (LLM)**을 사용하여 애플리케이션을 구축하기 위한 강력한 도구입니다. 쉽게 말해, **인공지능으로 구동되는 챗봇, 요약 도구, 질의응답 시스템 등을 만들 수 있도록 도와주는 도구**라고 할 수 있습니다. \\n\\nLangChain은 단순히 LLM을 사용하는 것 이상의 기능을 제공합니다. \\n\\n**LangChain의 주요 기능:**\\n\\n* **데이터 연동:** LLM이 외부 데이터와 상호 작용하도록 연결합니다. 즉, LLM이 특정 문서, 데이터베이스, API 등에서 정보를 가져와서 답변에 활용할 수 있도록 돕습니다.\\n* **체이닝:** 여러 LLM 호출을 순차적으로 연결하여 복잡한 작업을 수행합니다. 예를 들어, 텍스트를 요약하고, 번역하고, 질문에 답변하는 등의 작업을 하나의 체인으로 연결할 수 있습니다.\\n* **프롬프트 템플릿:** LLM에 입력으로 제공되는 프롬프트를 효율적으로 관리하고 최적화합니다. \\n* **에이전트:** LLM이 도구를 사용하고 환경과 상호 작용하여 작업을 완료하도록 합니다. 예를 들어, 계산기, 검색 엔진, 코드 실행기 등을 LLM이 필요에 따라 사용할 수 있도록 설정할 수 있습니다.\\n\\n**LangChain의 장점:**\\n\\n* **사용 편의성:** Python으로 작성되어 사용하기 쉬우며, 다양한 LLM 및 도구와의 통합을 지원합니다.\\n* **유연성:** 다양한 애플리케이션 요구 사항에 맞게 사용자 정의할 수 있는 유연한 프레임워크를 제공합니다.\\n* **확장성:**  LangChain을 사용하면 간단한 챗봇부터 복잡한 인공지능 시스템까지 다양한 규모의 애플리케이션을 구축할 수 있습니다.\\n\\n**LangChain 활용 예시:**\\n\\n* **챗봇:** 고객 지원, FAQ 답변, 개인 비서 등 다양한 용도의 챗봇을 구축할 수 있습니다.\\n* **요약 도구:** 긴 문서를 요약하거나, 뉴스 기사에서 주요 내용을 추출하는 등의 작업을 자동화할 수 있습니다.\\n* **질의응답 시스템:** 특정 문서나 데이터베이스에서 사용자의 질문에 대한 답변을 찾아 제공하는 시스템을 구축할 수 있습니다.\\n\\n**결론:**\\n\\nLangChain은 LLM의 기능을 최대한 활용하여 다양한 인공지능 애플리케이션을 구축할 수 있도록 돕는 강력한 도구입니다. 사용하기 쉽고 유연하며 확장 가능하다는 장점을 가지고 있어, 앞으로 더욱 많은 개발자들이 사용할 것으로 예상됩니다. \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_gemini.invoke(\"한국어로 LangChain 에 대해서 설명해줘\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gYhhibXZSGTF",
    "outputId": "3d276aa8-39a2-4b85-b9aa-5bb43729783b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain은 컴퓨터가 언어를 이해하고 대화할 수 있도록 도와주는 도구예요. 예를 들어, 우리가 친구와 이야기할 때 사용하는 언어처럼, LangChain은 컴퓨터가 사람과 대화할 수 있도록 도와주는 여러 가지 방법과 규칙을 가지고 있답니다. 그래서 LangChain을 사용하면 컴퓨터가 질문에 대답하거나 이야기를 나누는 것이 더 쉬워져요. 마치 친구와 놀 때 사용하는 장난감처럼, LangChain은 언어를 다루는 재미있는 도구예요!', response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 43, 'total_tokens': 164}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-8c866f2b-f5f9-49f4-ad3f-2649865e3225-0', usage_metadata={'input_tokens': 43, 'output_tokens': 121, 'total_tokens': 164})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# system과 human/user 메시지를 이용한 기본 요청\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "마치 다섯살 먹은 어린아이에게 설명하듯이 한국어로 쉽게 설명해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "LangChain이 무엇인가요?\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_prompt),\n",
    "]\n",
    "\n",
    "response = llm_gpt4.invoke(message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "AK7Xe8wnSQGa",
    "outputId": "c314481f-1ecb-4337-bf3c-08c274701b7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain은 컴퓨터가 언어를 이해하고 대화할 수 있도록 도와주는 도구예요. 예를 들어, 우리가 친구와 이야기할 때 사용하는 언어처럼, LangChain은 컴퓨터가 사람과 대화할 수 있도록 도와주는 여러 가지 방법과 규칙을 가지고 있답니다. 그래서 LangChain을 사용하면 컴퓨터가 질문에 대답하거나 이야기를 나누는 것이 더 쉬워져요. 마치 친구와 놀 때 사용하는 장난감처럼, LangChain은 언어를 다루는 재미있는 도구예요!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4i2vpQctaFA",
    "outputId": "03c03817-31c3-4faf-df60-2f2940748bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 컴퓨터가 언어를 이해하고 대화할 수 있도록 도와주는 도구예요. 예를 들어, 우리가 친구와 이야기할 때 사용하는 언어처럼, LangChain은 컴퓨터가 사람과\n",
      "대화할 수 있도록 도와주는 여러 가지 방법과 규칙을 가지고 있답니다. 그래서 LangChain을 사용하면 컴퓨터가 질문에 대답하거나 이야기를 나누는 것이 더 쉬워져요. 마치 친구와\n",
      "놀 때 사용하는 장난감처럼, LangChain은 언어를 다루는 재미있는 도구예요!\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "#텍스트를 지정된 너비에 맞게 줄 바꿈하여 하나의 문자열로 반환\n",
    "answer = textwrap.fill(response.content, width=100)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtMb5t7P0tRI",
    "outputId": "003b57d1-36a4-41e1-ad93-38d0034a1d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 우리가 인공지능 도우미를 만드는 걸 도와주는 도구 상자라고 생각하면 돼요.  여기엔 여러 가지 블록들이 있어요. 이 블록들을 맞추면 우리가 원하는 인공지능\n",
      "도우미를 만들 수 있죠.  예를 들어, 어떤 블록은 우리가 쓴 글을 이해할 수 있게 해줘요. 또 어떤 블록은 이해한 내용으로 답변을 만들어 내죠.  이런 블록들을 연결해서 우리가\n",
      "물어보면 알맞은 대답을 해주는 똑똑한 인공지능 친구를 만들 수 있어요.  이 블록들은 마치 장난감 블록처럼 서로 잘 맞물려요. 그래서 우리는 블록을 바꿔가며 다양한 능력을 가진\n",
      "인공지능 도우미를 쉽게 만들 수 있죠.  LangChain이 바로 이런 블록들을 모아놓은 도구 상자에요. 우리는 LangChain 안에 있는 블록을 가지고 놀면서 멋진 인공지능\n",
      "친구를 만들 수 있답니다!\n"
     ]
    }
   ],
   "source": [
    "response = llm_claude3.invoke(message)\n",
    "\n",
    "answer = textwrap.fill(response.content, width=100)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fi6_NvW3MaE"
   },
   "source": [
    "## 2. Chain, Prompt 및 Loader\n",
    "\n",
    "- LLM에서의 Chain 은 Data Processing 에서의 Pipeline 과 유사한 개념\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G447ezl5X5ye"
   },
   "source": [
    "### 체인의 구성 요소 - Runnables\n",
    "- 프롬프트 : LLM의 응답을 안내하는 템플릿  \n",
    "- LLM 또는 채팅 모델 : 프롬프트에 따라 응답을 생성하는 엔진  \n",
    "- 출력 파서 : LLM의 출력을 파싱하는 도구  \n",
    "- 도구 : LLM이 API에서 추가 정보를 추출하거나 코드를 실행하여 LLM을 에이전트로 전환할 수 있게 해주는 확장 기능  \n",
    "- 일반 함수 : 서로 연결될 수 있는 추가적인 일반 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Yt1j9kXR1h53"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# simple prompt template 생성\n",
    "# {topic} 변수에서 사용자의 query 가 대체된다.\n",
    "prompt_template = \"\"\"\n",
    "    You are a helpful assistant that explains AI topics. Given the following input:\n",
    "    {topic}\n",
    "    Provide an explanation of the given topic.\n",
    "\"\"\"\n",
    "\n",
    "# prompt template 을 이용하여 prompt 생성\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "0vikrfE53yDk",
    "outputId": "adf40012-9025-4a1a-a591-da9fd5b4883c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain은 자연어 처리(NLP)와 관련된 애플리케이션을 개발하기 위한 프레임워크입니다. 주로 대화형 AI 시스템, 챗봇, 그리고 기타 언어 기반 애플리케이션을 만드는 데 사용됩니다. LangChain은 다양한 언어 모델과 통합하여, 사용자가 원하는 기능을 쉽게 구현할 수 있도록 돕습니다.\\n\\nLangChain의 주요 특징은 다음과 같습니다:\\n\\n1. **모듈화**: LangChain은 다양한 구성 요소로 나뉘어 있어, 개발자가 필요에 따라 특정 모듈만 선택하여 사용할 수 있습니다. 예를 들어, 텍스트 생성, 대화 관리, 데이터베이스 연동 등의 기능을 모듈별로 구현할 수 있습니다.\\n\\n2. **다양한 언어 모델 지원**: LangChain은 OpenAI의 GPT 계열 모델, Hugging Face의 Transformers 등 여러 언어 모델과 연동될 수 있어, 개발자는 원하는 AI 모델을 쉽게 선택할 수 있습니다.\\n\\n3. **유연성**: LangChain은 사용자 정의 로직을 추가할 수 있는 유연성을 제공하여, 특정 비즈니스 요구사항이나 사용자 경험에 맞게 기능을 커스터마이즈할 수 있습니다.\\n\\n4. **데이터 소스 통합**: LangChain은 외부 데이터 소스와의 통합을 지원하여, AI 모델이 더 많은 정보를 바탕으로 답변할 수 있도록 합니다. 예를 들어, 데이터베이스, API 등을 통해 실시간 정보를 제공받을 수 있습니다.\\n\\n5. **커뮤니티와 지원**: LangChain은 오픈 소스 프로젝트로, 활발한 커뮤니티가 있어 문서, 튜토리얼, 예제 코드 등을 통해 개발자들이 쉽게 배울 수 있도록 돕습니다.\\n\\n이와 같은 특징 덕분에 LangChain은 다양한 산업 분야에서 사용자 맞춤형 언어 기반 솔루션을 구축하는 데 널리 사용되고 있습니다.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe operator \"|\"\" 를 이용하여 하나 이상의 chain 을 결합\n",
    "chain = prompt | llm_gpt4\n",
    "\n",
    "chain.invoke({\"topic\": \"LangChain이 뭐예요?\"}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJzXNZCnZRCr"
   },
   "source": [
    "더 고급 체인을 만들기 위해 LangChain을 사용하여 YouTube 비디오를 필사합니다. 먼저 YouTube Transcript API를 설치합니다. 그런 다음 LangChain의 커뮤니티 문서 로더에서 YouTube Loader를 가져옵니다. 로더에 비디오 URL을 전달하면 필사본을 추출할 수 있으며, 이는 문서 목록으로 반환됩니다. 원시 필사본을 얻으려면 이러한 문서에서 페이지 콘텐츠를 추출하거나 문서를 체인에 직접 전달하기만 하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1HMGE6k4mQp"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3r04ZHou46-F"
   },
   "outputs": [],
   "source": [
    "# LangChain 커뮤니티에서 제공하는 Youtube Loader를 임포트\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "# YoutubeLoader를 사용하여 유튜브 URL에서 로더를 생성\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=AOEGOhkGtjI\", add_video_info=False\n",
    ")\n",
    "\n",
    "# 비디오의 자막을 문서로 로드\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kiecolwv5jen",
    "outputId": "aa1c7ad4-6648-49b7-dec9-74197dd8920f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama 3 and some of the capabilities of the model you can see how the model compares to other popular llms specifically the 70b model that I'm going to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain all right the first thing I'm going to do is I'm going to set up the cop notebook and pip install the required libraries then I'll connect to Bay and fetch the schema information from tables in a data set I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions that extracts the schema information from bit query in a schema. py file and this is the same functions that I've been using in the earlier videos that lets me extract and feed the scheme information from bitre to the chain all right so I'm just going to load the environment variables and then I'm going to connect to biy and the data set I'm using is the same data set I used in the last video in the dashboard video it is an e-commerce data set with four tables customers orders products and customer taxs and the two functions in the schema. py file allows me to extract the schema information from the data set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this case I'm using Lama 370b and the prom template will be injecting three things first I'll inject the schema information I'm extracting from the bit crate data set then I'm injecting the main question or the query and finally I'll be injecting a a message history and the message history is The Tweak I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch the error and feed it back to the chain and this allows me to make the chains self-correcting which is useful when we're dealing with a model that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to to inject the schema information and to inject the messages of the message history that contains the errors then I use my prompt the language model and a string output passer and this is what we need to generate the SQL code from a prompt now let's move on to generate some insights with this setup I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets me extract the SQL code from the response and in this extract SQL function I'm simply using regex to extract the SQL from whatever the language model is returning to wrap it all up I'm creating a function that takes a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error I'm going to collect the error and feed it back to the chain and try again and in this way the chain will be self-corrected ing because the llm will understand the error message okay so let's try this the first prompt I'm going to give L three is the following give me a list of the best customers including their rank their first name last name and email and the products they purchased and this one it got in the first attempt so the query executed successfully and we can then have a look at the data frame and this is essentially an audience that you could use for marketing purposes you normally create an audience like this in a customer data platform now let's try a different one I'll do a classical one show me the revenue generated in the last 30 days broken down by acquisition Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the error message makes the second attempt successful and here we have Revenue broken down by acquisition source let's do another audience let's say I want the top 100 customers with the highest purchase frequency but with an under average aov and again we see that the first attempt fails and the second attempt is successful and here we only got the names let's say that we want to include the frequency and the aov as well to get the full overview and here we see that the first attempt fails the second attempt also fails but the third attempt is a success and here we have the full audience data frame with the purchase frequency and the average order value so this is very useful so we can use llama 3 for generating insights if we just implement this small tweak of catching the errors and feeding it back to the chain all right so what are the implications of this first of all we now know that with Lama 3 open source llms can be used to generate insights and this is very good news for privacy sensitive use cases so use cases where you want to feed sensitive customer data back to the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use Lama 3 locally this is also very good news for query heavy applications so text tosql applications can be query heavy if they're rolled out in a big organization so there's a cost consideration that might be worth looking into now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really useful for Consumer facing applications where speed is necessary finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm generated in the future and not so distant future so if you are a data analyst or data engineer you should really pay attention to this and learn this new technology all right that's it for now if you enjoyed this video I suggest you check out one of the other videos on generating SQL with llm chains thanks for watching\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "2Nh1VYeQ5tdt",
    "outputId": "ea0d5e86-a0bd-49f7-b7f4-0721bfedac22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama 3 and some of the capabilities of the model you can see how the model compares to other popular llms specifically the 70b model that I'm going to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain all right the first thing I'm going to do is I'm going to set up the cop notebook and pip install the required libraries then I'll connect to Bay and fetch the schema information from tables in a data set I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions that extracts the schema information from bit query in a schema. py file and this is the same functions that I've been using in the earlier videos that lets me extract and feed the scheme information from bitre to the chain all right so I'm just going to load the environment variables and then I'm going to connect to biy and the data set I'm using is the same data set I used in the last video in the dashboard video it is an e-commerce data set with four tables customers orders products and customer taxs and the two functions in the schema. py file allows me to extract the schema information from the data set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this case I'm using Lama 370b and the prom template will be injecting three things first I'll inject the schema information I'm extracting from the bit crate data set then I'm injecting the main question or the query and finally I'll be injecting a a message history and the message history is The Tweak I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch the error and feed it back to the chain and this allows me to make the chains self-correcting which is useful when we're dealing with a model that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to to inject the schema information and to inject the messages of the message history that contains the errors then I use my prompt the language model and a string output passer and this is what we need to generate the SQL code from a prompt now let's move on to generate some insights with this setup I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets me extract the SQL code from the response and in this extract SQL function I'm simply using regex to extract the SQL from whatever the language model is returning to wrap it all up I'm creating a function that takes a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error I'm going to collect the error and feed it back to the chain and try again and in this way the chain will be self-corrected ing because the llm will understand the error message okay so let's try this the first prompt I'm going to give L three is the following give me a list of the best customers including their rank their first name last name and email and the products they purchased and this one it got in the first attempt so the query executed successfully and we can then have a look at the data frame and this is essentially an audience that you could use for marketing purposes you normally create an audience like this in a customer data platform now let's try a different one I'll do a classical one show me the revenue generated in the last 30 days broken down by acquisition Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the error message makes the second attempt successful and here we have Revenue broken down by acquisition source let's do another audience let's say I want the top 100 customers with the highest purchase frequency but with an under average aov and again we see that the first attempt fails and the second attempt is successful and here we only got the names let's say that we want to include the frequency and the aov as well to get the full overview and here we see that the first attempt fails the second attempt also fails but the third attempt is a success and here we have the full audience data frame with the purchase frequency and the average order value so this is very useful so we can use llama 3 for generating insights if we just implement this small tweak of catching the errors and feeding it back to the chain all right so what are the implications of this first of all we now know that with Lama 3 open source llms can be used to generate insights and this is very good news for privacy sensitive use cases so use cases where you want to feed sensitive customer data back to the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use Lama 3 locally this is also very good news for query heavy applications so text tosql applications can be query heavy if they're rolled out in a big organization so there's a cost consideration that might be worth looking into now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really useful for Consumer facing applications where speed is necessary finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm generated in the future and not so distant future so if you are a data analyst or data engineer you should really pay attention to this and learn this new technology all right that's it for now if you enjoyed this video I suggest you check out one of the other videos on generating SQL with llm chains thanks for watching\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = docs[0].page_content\n",
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEScwvtSck_C"
   },
   "source": [
    "이제, 대본이 주어진 YouTube 비디오를 요약하는 체인을 설정해 보겠습니다. 비디오 대본을 입력 변수로 프롬프트 템플릿에 주입한 다음 파이프 연산자를 사용하여 체인을 설정합니다. 비디오 대본으로 호출하면 원시 텍스트를 수동으로 추출하지 않고도 간결한 요약을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyEU9cQo7kZF",
    "outputId": "40c8d37c-dffe-40f2-caab-0d0348e7e0e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['video_transcript'], template='\\n    You are a helpful assistant that explains YT videos. Given the following video transcript:\\n    {video_transcript}\\n    한국어로 요약해 주세요.\\n')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전사된 내용을 chain에 사용합니다.\n",
    "# prompt template 생성\n",
    "prompt_template = \"\"\"\n",
    "    You are a helpful assistant that explains YT videos. Given the following video transcript:\n",
    "    {video_transcript}\n",
    "    한국어로 요약해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "# prompt instance 생성\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"video_transcript\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MXvV726M8VPN"
   },
   "outputs": [],
   "source": [
    "# chain 생성\n",
    "chain = prompt | llm_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OW1qGhFF8e3f",
    "outputId": "d64990be-176c-4715-e60b-4331a8f208bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 비디오에서는 Meta의 Lama 3 모델을 사용하여 데이터 분석에서 LLM(대형 언어 모델)의 성능을 극대화하는 방법에 대해 설명하고 있습니다. 사용자는 Gro Cloud에서 Lama 3를 설정하고 SQL 체인을 구축하며, SQL 코드 생성을 최적화하는 추가 조치를 취합니다. 비디오는 Lama 3가 매우 빠르고 고급 SQL을 생성할 수 있으며, 이를 통해 데이터 인사이트를 추출하는 과정도 보여줍니다.\n",
      "\n",
      "사용자는 e-commerce 데이터 세트를 사용하여 SQL 쿼리를 실행하고, 오류가 발생할 경우 이를 모델에 피드백하여 체인을 자동으로 수정하는 방법을 설명합니다. 이를 통해 사용자는 고객 리스트, 수익 분석 등 다양한 인사이트를 성공적으로 생성합니다.\n",
      "\n",
      "이 비디오의 결론은 Lama 3와 같은 오픈 소스 LLM이 데이터 분석에서 유용하게 사용될 수 있으며, 특히 민감한 고객 데이터를 다룰 때 유용하다는 점입니다. 데이터 분석가나 데이터 엔지니어는 이러한 새로운 기술을 배우는 것이 중요하다고 강조합니다.\n"
     ]
    }
   ],
   "source": [
    "answer = chain.invoke({\"video_transcript\": docs}).content\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWWyBVnI82BW"
   },
   "source": [
    "### Built-in Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QG0H9E858fh0"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# create_stuff_documents_chain은 문서 목록을 받아 모두 하나의 프롬프트로 형식화합니다.\n",
    "# create_stuff_documents_chain 의 input variable 이름은 context 여야 합니다.\n",
    "prompt_template = \"\"\"\n",
    "    You are a helpful assistant that explains YT videos. Given the following video transcript:\n",
    "    {context}\n",
    "    한국어로 요약해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "# prompt instance 생성\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# chain 생성\n",
    "chain = create_stuff_documents_chain(llm_gpt4, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "uAuC8u_--akr",
    "outputId": "ac21eafa-1390-4c7d-c289-3ebabdf1f2d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이 영상에서는 메타의 새로운 모델인 Lama 3에 대해 설명하고 있습니다. 이 모델은 LLM(대형 언어 모델)을 사용한 데이터 분석에 있어 혁신적인 변화를 가져올 것으로 기대됩니다. 영상에서는 Lama 3를 Gro Cloud에서 사용하여 SQL 체인을 최적화하는 방법을 보여주고, 이 모델이 추출할 수 있는 통찰력에 대해 설명합니다.\\n\\n주요 내용은 다음과 같습니다:\\n\\n1. **Lama 3의 성능**: Lama 3는 텍스트에서 SQL로의 변환이 매우 빠르며, 고급 SQL을 생성할 수 있습니다. 추가적인 조정을 통해 성능을 극대화할 수 있습니다.\\n\\n2. **환경 설정**: Python 환경을 설정하고 필요한 라이브러리를 설치한 뒤, 데이터셋의 스키마 정보를 가져옵니다. 이 데이터셋은 고객, 주문, 제품 및 고객 세금 관련 테이블로 구성되어 있습니다.\\n\\n3. **SQL 체인 설정**: Lang Chain을 사용하여 Lama 3를 Gro Cloud에 연결하고, SQL 코드를 생성합니다. 에러가 발생할 경우 이를 처리하여 체인을 수정하는 방법을 사용합니다.\\n\\n4. **데이터 분석 예시**: 여러 SQL 쿼리를 실행하며, 오류를 피드백하여 성공적인 실행을 도와주는 방법을 시연합니다. 예를 들어, 고객 목록을 생성하거나 최근 30일간의 수익을 분석하는 쿼리를 실행합니다.\\n\\n5. **미래 전망**: Lama 3와 같은 오픈 소스 LLM을 통해 데이터 분석에서의 통찰력을 생성할 수 있는 가능성이 제시됩니다. 이는 프라이버시가 중요한 경우나 쿼리 집약적인 응용 프로그램에서 유용할 것입니다.\\n\\n영상은 데이터 분석가나 데이터 엔지니어에게 LLM 기술을 학습할 것을 권장하며 마무리됩니다.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\": docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 LangChain Expression Language(LCEL) & Runnables\n",
    "\n",
    "- LCEL(LangChain Expression Language) 는 Runnables 를 chain 으로 구성하는 방법\n",
    "\n",
    "LCEL은 기본 구성 요소에서 복잡한 체인을 구축하는 것을 간소화합니다. 파이프 연산자(|)를 사용하여 다양한 구성 요소를 체인으로 연결하고 한 요소에서 다음 요소로 출력을 공급합니다. 이런 방식으로 구성된 체인의 간단한 예로는 모델과 출력 파서가 결합된 프롬프트가 있습니다.  이러한 구성 요소들을 runnables 라고 부릅니다.  \n",
    "\n",
    "`chain=prompt | model | output_parser`\n",
    "\n",
    "- Chain 구성  \n",
    "    - 체인에 대한 입력(일반적으로 사전)\n",
    "    - 입력은 프롬프트로 전송됩니다.\n",
    "    - 프롬프트 값은 LLM 또는 채팅 모델로 전송됩니다.\n",
    "    - Chatmodel이 채팅 메시지를 반환합니다.\n",
    "    - 파서는 채팅 메시지에서 문자열을 추출합니다.\n",
    "    - 문자열은 체인의 출력입니다\n",
    "\n",
    "- LangChain의 runnable 객체들:\n",
    "\n",
    "    - RunnableSequence : 여러 runnable 구성 요소를 연결하여 각 구성 요소가 입력을 처리하고 출력을 다음 구성 요소에 전달\n",
    "    - RunnableLambda : Python의 호출 가능한 요소(함수 등)를 실행 가능한 구성 요소로 바꿔서 체인으로 통합\n",
    "    - RunnablePassthrough : 입력을 변경하지 않고 통과시키거나 출력에 추가 키를 추가. placeholder 역할을 하거나 시퀀스에 유연하게 통합할 수 있다.\n",
    "    - RunnableParallel : 여러 개의 실행 파일을 동시에 실행하여 두 개의 체인이 동일한 입력에서 실행되지만 다른 출력을 반환하는 분기를 허용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3s0DaSYO_W0Y"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "summarize_prompt_template = \"\"\"\n",
    "    You are a helpful assistant that summarizes AI concepts:\n",
    "    {context}\n",
    "    context를 한국어로 요약해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = PromptTemplate.from_template(summarize_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InN3aD1qGwmD"
   },
   "source": [
    "###  \"|\" 연산자를 이용하여 Runnable Sequence chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0cahXhesl-E",
    "outputId": "eed661f4-9190-4da5-f8df-0139d0df7b64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "WpJAZmpOsjHA",
    "outputId": "147f9a53-6d84-4d3c-cae6-4322635e23f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain은 자연어 처리(NLP)와 관련된 다양한 애플리케이션을 구축하기 위해 설계된 프레임워크입니다. 이 프레임워크는 언어 모델을 사용하여 대화형 에이전트, 챗봇, 정보 검색 시스템 등을 개발하는 데 도움을 줍니다. LangChain은 다양한 구성 요소를 통합하여 최적의 사용자 경험을 제공하며, 데이터 소스와의 상호작용, 사용자 입력 처리, 출력 생성 등을 효율적으로 관리합니다.\\n\\n주요 기능으로는 언어 모델과의 연결, 데이터베이스 쿼리, API 호출, 메모리 관리 등을 포함하며, 이를 통해 개발자들은 복잡한 NLP 시스템을 보다 쉽게 구축할 수 있습니다. LangChain은 모듈화된 접근 방식을 취하고 있어, 필요에 따라 각 구성 요소를 조정하거나 확장할 수 있습니다.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_VLKGZ1HO7d"
   },
   "source": [
    "### RunnableLambda 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oI9MZ0sasxQf",
    "outputId": "c5356d5c-6811-43af-9bfb-31fe88e2638e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'langchain_core.runnables.base.RunnableLambda'>\n",
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# RunnableLambda를 사용하여 Python 함수를 체인에 삽입합니다.\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "print(type(summarize_chain))\n",
    "\n",
    "# 사용자 정의 람다 함수를 정의하고 이를 RunnableLambda에 래핑합니다.\n",
    "length_lambda = RunnableLambda(lambda summary: f\"Summary length: {len(summary)} characters\")\n",
    "print(type(length_lambda))\n",
    "\n",
    "lambda_chain = summarize_chain | length_lambda\n",
    "print(type(lambda_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TAa1W6iVHLnq",
    "outputId": "5b181616-a4f7-49a1-b807-9795fc389b9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summary length: 503 characters'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhIbYeOgIYyi",
    "outputId": "6ee972ca-751b-4b86-a8cd-fefe6fbe8c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context'] template='\\n    You are a helpful assistant that summarizes AI concepts:\\n    {context}\\n    context를 한국어로 요약해 주세요.\\n'\n",
      "client=<openai.resources.chat.completions.Completions object at 0x000001F7866375E0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001F7866495E0> model_name='gpt-4o-mini-2024-07-18' openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "\n",
      "RunnableLambda(lambda summary: f'Summary length: {len(summary)} characters')\n"
     ]
    }
   ],
   "source": [
    "for step in lambda_chain.steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8q2V33ttqTg"
   },
   "source": [
    "함수를 명시적으로 RunnableLambda로 변환하지 않고도 체인에서 함수를 사용할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydx2GKyENIna",
    "outputId": "b53816bc-77c8-4e11-de5f-9362b79bc6ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda summary: f'Summary Length: {len(summary)} characters')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RunnableLambda로 변환하지 않고 체인에서 함수 사용\n",
    "chain_without_function = summarize_chain | (lambda summary: f\"Summary Length: {len(summary)} characters\")\n",
    "\n",
    "chain_without_function.steps[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "00FqsywiNyq6",
    "outputId": "019c1f51-59e1-4b93-fc51-85d18097db15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summary Length: 310 characters'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_without_function.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktwM5hWrOTvQ"
   },
   "source": [
    "### placeholder 로 사용되는 RunnablePassthrough\n",
    "\n",
    "체인 내에서 데이터를 변경하지 않고 다음 단계로 전달하는 데 사용됩니다. 주로 디버깅하거나 체인의 특정 부분을 테스트할 때 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cnTjJX9OOOPj",
    "outputId": "72c6fc6b-540c-4ad4-ae24-b473f9941c09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summary length: 272 characters'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "\n",
    "# RunnablePassthrough instance 생성\n",
    "passthrough = RunnablePassthrough()\n",
    "\n",
    "# 요약 및 길이 계산과 함께 파이프 연산자를 사용하여 시퀀스 생성\n",
    "placeholder_chain = summarize_chain | passthrough | length_lambda\n",
    "\n",
    "placeholder_chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZCrH_wbPRrw",
    "outputId": "4ee15bed-aafd-45d7-e976-785d274c6a5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptTemplate(input_variables=['context'], template='\\n    You are a helpful assistant that summarizes AI concepts:\\n    {context}\\n    context를 한국어로 요약해 주세요.\\n'),\n",
       " ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001F7866375E0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001F7866495E0>, model_name='gpt-4o-mini-2024-07-18', openai_api_key=SecretStr('**********'), openai_proxy=''),\n",
       " StrOutputParser(),\n",
       " RunnablePassthrough(),\n",
       " RunnableLambda(lambda summary: f'Summary length: {len(summary)} characters')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_chain.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOnoJI94Pu0Z"
   },
   "source": [
    "### RunnablePassthrough for assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fb-G8U7CPY_p",
    "outputId": "14a58845-690c-4989-96b5-9e796e6c1789"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'LangChain은 자연어 처리(NLP) 및 인공지능(AI) 애플리케이션을 구축하기 위한 프레임워크입니다. 이 프레임워크는 다양한 언어 모델과 데이터 소스를 통합하여 복잡한 작업을 수행할 수 있도록 돕습니다. LangChain의 주요 구성 요소는 다음과 같습니다:\\n\\n1. **체인(Chain)**: 여러 작업을 연결하여 복잡한 프로세스를 생성하는 구조입니다.\\n2. **프롬프트(Prompt)**: 언어 모델에 주어지는 입력입니다. 프롬프트를 잘 설계하는 것이 결과의 품질에 큰 영향을 미칩니다.\\n3. **메모리(Memory)**: 대화의 맥락을 유지하기 위해 사용되는 구성 요소로, 이전의 대화 내용을 기억합니다.\\n4. **데이터 소스(Data Sources)**: 외부 데이터베이스나 API와 연결하여 정보를 검색하거나 활용하는 기능입니다.\\n\\nLangChain은 이러한 요소들을 조합하여 대화형 AI, 질문 응답 시스템, 텍스트 생성 등 다양한 애플리케이션을 쉽게 개발할 수 있도록 지원합니다.',\n",
       " 'length': 501}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약을 dictionary로 변환하는 사용자 정의 람다 함수 정의\n",
    "wrap_summary_lambda = RunnableLambda(lambda summary: {\"summary\": summary})\n",
    "\n",
    "# Dict 입력에 mapping argument를 merge RunnablePassthrough 인스턴스 생성\n",
    "assign_passthrough = RunnablePassthrough.assign(length=lambda x: len(x['summary']))\n",
    "\n",
    "# summarization chain 생성\n",
    "summarize_chain = summarize_prompt | llm_gpt4 | output_parser | wrap_summary_lambda\n",
    "\n",
    "# 요약과 assign_passthrough를 결합하여 전체 체인을 생성\n",
    "assign_chain = summarize_chain | assign_passthrough\n",
    "\n",
    "# Use the chain\n",
    "result = assign_chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfAvpNwmxGga",
    "outputId": "f0bb779e-40d0-4369-a8e1-31c33029a6df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.passthrough.RunnableAssign'>\n"
     ]
    }
   ],
   "source": [
    "print(type(assign_chain.steps[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87QUb996RXRC"
   },
   "source": [
    "### RunnableParallel 사용 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flTnjKw1QykS",
    "outputId": "9dd1775f-1a43-4ae2-beaf-11257781a9d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'summary': 'LangChain은 자연어 처리(NLP) 애플리케이션을 구축하기 위한 프레임워크로, 다양한 언어 모델과 데이터 소스를 통합하여 복잡한 작업을 수행할 수 있도록 돕습니다. 이 프레임워크는 언어 모델을 사용하여 질문에 답하거나 정보를 생성하는 등의 작업을 쉽게 구현할 수 있게 해줍니다. LangChain은 데이터 체인, 프롬프트 템플릿, 메모리 관리 등 여러 기능을 제공하여 개발자들이 더 효율적으로 작업할 수 있도록 지원합니다. \\n\\n이러한 기능들은 AI 기반 애플리케이션을 개발하는 데 있어 유연성과 확장성을 제공하며, 다양한 비즈니스 요구에 맞춰 활용될 수 있습니다. LangChain은 특히 대화형 AI, 챗봇, 자동화된 고객 서비스 등 다양한 분야에서 사용될 수 있습니다.'},\n",
       " 'length': 1}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "#summarization chain 생성\n",
    "summarization_chain = summarize_prompt | llm_gpt4 | output_parser\n",
    "\n",
    "# summary 와 길이 계산을 병렬로 처리하기 위해 RunnableParallel 인스턴스를 만듭니다.\n",
    "parallel_runnable = RunnableParallel(\n",
    "    summary=lambda x: x,  # 현재의 summary 전달\n",
    "    length=lambda x: len(x)   # summary의 길이 계산\n",
    ")\n",
    "\n",
    "# parellel runnable과 summarization 결합\n",
    "parallel_chain = summarize_chain | parallel_runnable\n",
    "\n",
    "parallel_chain.invoke({\"context\": \"LangChain 에 대해 설명해 줘\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHihnjNGSXq9"
   },
   "source": [
    "### 4. Splitter & Retrievers\n",
    "\n",
    "LangChain은 검색기 및 분할기를 포함하여 문서 작업을 위한 강력한 도구를 제공합니다. 검색기는 주어진 쿼리에 따라 관련 문서를 검색하고 검색하는 데 사용되는 반면, 분할기는 큰 문서를 더 작고 관리하기 쉬운 청크로 나누는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "DiS-rjjjSMza"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yz9g6kVKULE7",
    "outputId": "1b039c87-bbb2-42b8-d774-2ffa4133e398"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama 3 and some of the capabilities of the model you can see how the model compares to other popular llms specifically the 70b model that I'm going to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain all right the first thing I'm going to do is I'm going to set up the cop notebook and pip install the required libraries then I'll connect to Bay and fetch the schema information from tables in a data set I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions that extracts the schema information from bit query in a schema. py file and this is the same functions that I've been using in the earlier videos that lets me extract and feed the scheme information from bitre to the chain all right so I'm just going to load the environment variables and then I'm going to connect to biy and the data set I'm using is the same data set I used in the last video in the dashboard video it is an e-commerce data set with four tables customers orders products and customer taxs and the two functions in the schema. py file allows me to extract the schema information from the data set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this case I'm using Lama 370b and the prom template will be injecting three things first I'll inject the schema information I'm extracting from the bit crate data set then I'm injecting the main question or the query and finally I'll be injecting a a message history and the message history is The Tweak I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch the error and feed it back to the chain and this allows me to make the chains self-correcting which is useful when we're dealing with a model that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to to inject the schema information and to inject the messages of the message history that contains the errors then I use my prompt the language model and a string output passer and this is what we need to generate the SQL code from a prompt now let's move on to generate some insights with this setup I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets me extract the SQL code from the response and in this extract SQL function I'm simply using regex to extract the SQL from whatever the language model is returning to wrap it all up I'm creating a function that takes a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error I'm going to collect the error and feed it back to the chain and try again and in this way the chain will be self-corrected ing because the llm will understand the error message okay so let's try this the first prompt I'm going to give L three is the following give me a list of the best customers including their rank their first name last name and email and the products they purchased and this one it got in the first attempt so the query executed successfully and we can then have a look at the data frame and this is essentially an audience that you could use for marketing purposes you normally create an audience like this in a customer data platform now let's try a different one I'll do a classical one show me the revenue generated in the last 30 days broken down by acquisition Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the error message makes the second attempt successful and here we have Revenue broken down by acquisition source let's do another audience let's say I want the top 100 customers with the highest purchase frequency but with an under average aov and again we see that the first attempt fails and the second attempt is successful and here we only got the names let's say that we want to include the frequency and the aov as well to get the full overview and here we see that the first attempt fails the second attempt also fails but the third attempt is a success and here we have the full audience data frame with the purchase frequency and the average order value so this is very useful so we can use llama 3 for generating insights if we just implement this small tweak of catching the errors and feeding it back to the chain all right so what are the implications of this first of all we now know that with Lama 3 open source llms can be used to generate insights and this is very good news for privacy sensitive use cases so use cases where you want to feed sensitive customer data back to the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use Lama 3 locally this is also very good news for query heavy applications so text tosql applications can be query heavy if they're rolled out in a big organization so there's a cost consideration that might be worth looking into now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really useful for Consumer facing applications where speed is necessary finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm generated in the future and not so distant future so if you are a data analyst or data engineer you should really pay attention to this and learn this new technology all right that's it for now if you enjoyed this video I suggest you check out one of the other videos on generating SQL with llm chains thanks for watching\")]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  YoutubeLoader는 YouTube 비디오의 자막을 가져와서 문서로 변환합니다.\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=AOEGOhkGtjI\", add_video_info=False\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "nzeqXMvDUkWp"
   },
   "outputs": [],
   "source": [
    "# 텍스트를 재귀적으로 분할하여 지정된 길이의 청크로 분할\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-cJ6TCEWWsW",
    "outputId": "3e2e3a0a-7c70-4639-9f3b-2c4258efa202"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='so now we have Lama 3 from meta and this model is definitely going to be a GameChanger when it comes'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"are the text to SQL chains blazing fast they're also capable of generating quite Advanced SQL and\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='Advanced SQL and almost on par with the high IQ llms if we Implement one additional tweak so in'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"tweak so in this video I'm going to show you how we can tweak the SQL chains to maximize the\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"to maximize the performance of Lama 3 we're going to have a look at some of the insights that Lama\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"insights that Lama three is capable of extracting and finally I'm going to briefly discuss some of\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='can read about Lama 3 and some of the capabilities of the model you can see how the model compares'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"the model compares to other popular llms specifically the 70b model that I'm going to be using in\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"to be using in this video is compared to Gemini and CLA 3 Sunnet and there's also a link that lets\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"a link that lets you request access to Lama 3 but this is not what I'll be doing I'm going to be\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"I'm going to be using Gro Cloud because it's the fastest and easiest way to get started using llama\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"started using llama 3 and as you can see the 70b model is already available on gr Cloud so I'll\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"on gr Cloud so I'll just grab the API key and then I'm ready to build the SQL chains with L chain\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"chains with L chain all right the first thing I'm going to do is I'm going to set up the cop\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"to set up the cop notebook and pip install the required libraries then I'll connect to Bay and\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"connect to Bay and fetch the schema information from tables in a data set I'll be installing\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"I'll be installing python. EnV to fetch the API Keys Lang chain Gro the Lang chain Gro connector\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='chain Gro connector and Google Cloud B quy then I have uploaded myv file with the API keys I have'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='the API keys I have my Google cloud service account key DBQ key. jjon then I have two functions'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='have two functions that extracts the schema information from bit query in a schema. py file and'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"schema. py file and this is the same functions that I've been using in the earlier videos that lets\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='videos that lets me extract and feed the scheme information from bitre to the chain all right so'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"chain all right so I'm just going to load the environment variables and then I'm going to connect\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"going to connect to biy and the data set I'm using is the same data set I used in the last video in\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='the last video in the dashboard video it is an e-commerce data set with four tables customers'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='tables customers orders products and customer taxs and the two functions in the schema. py file'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='the schema. py file allows me to extract the schema information from the data set as you can see'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"set as you can see here so now let's connect to Lama 3 on Gro cloud and set set up the SQL chain\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='up the SQL chain using Lang chain expression language to connect to Lama 3 on groc cloud we import'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='cloud we import chat Gro from Lang chain grock and then we instantiate it with a model name in this'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"model name in this case I'm using Lama 370b and the prom template will be injecting three things\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"three things first I'll inject the schema information I'm extracting from the bit crate data set\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"bit crate data set then I'm injecting the main question or the query and finally I'll be injecting\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"I'll be injecting a a message history and the message history is The Tweak I mentioned in the\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"I mentioned in the beginning so I'm going to have Lama 3 generate SQL code and then I'll use the\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"then I'll use the bitr client to execute that SQL code and if there's an error I'm going to catch\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"I'm going to catch the error and feed it back to the chain and this allows me to make the chains\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"to make the chains self-correcting which is useful when we're dealing with a model that is of a low\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"that is of a low IQ the SQL chain is assembled in the usual way I'll use a runnable path through to\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='path through to to inject the schema information and to inject the messages of the message history'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='the message history that contains the errors then I use my prompt the language model and a string'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='model and a string output passer and this is what we need to generate the SQL code from a prompt'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"code from a prompt now let's move on to generate some insights with this setup I had difficulties\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='I had difficulties having Lama 3 return clean executable SQL so I had to write a function that lets'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"function that lets me extract the SQL code from the response and in this extract SQL function I'm\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"SQL function I'm simply using regex to extract the SQL from whatever the language model is\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"language model is returning to wrap it all up I'm creating a function that takes a prompt and a\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='a prompt and a number of attempts as input and then it will generate the SQL using the Lang chain'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"the Lang chain SQL chain and try to execute that SQL up to five times and whenever there's an error\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"there's an error I'm going to collect the error and feed it back to the chain and try again and in\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='try again and in this way the chain will be self-corrected ing because the llm will understand the'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"will understand the error message okay so let's try this the first prompt I'm going to give L three\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='to give L three is the following give me a list of the best customers including their rank their'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='their rank their first name last name and email and the products they purchased and this one it got'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='and this one it got in the first attempt so the query executed successfully and we can then have a'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='we can then have a look at the data frame and this is essentially an audience that you could use'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='that you could use for marketing purposes you normally create an audience like this in a customer'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"this in a customer data platform now let's try a different one I'll do a classical one show me the\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='one show me the revenue generated in the last 30 days broken down by acquisition Channel and here'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='Channel and here you can see that the first attempt is unsuccessful it fails but feeding back the'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='feeding back the error message makes the second attempt successful and here we have Revenue broken'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"have Revenue broken down by acquisition source let's do another audience let's say I want the top\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='say I want the top 100 customers with the highest purchase frequency but with an under average aov'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='under average aov and again we see that the first attempt fails and the second attempt is'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"second attempt is successful and here we only got the names let's say that we want to include the\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='want to include the frequency and the aov as well to get the full overview and here we see that the'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='we see that the first attempt fails the second attempt also fails but the third attempt is a'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='third attempt is a success and here we have the full audience data frame with the purchase'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='with the purchase frequency and the average order value so this is very useful so we can use llama'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='so we can use llama 3 for generating insights if we just implement this small tweak of catching the'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='of catching the errors and feeding it back to the chain all right so what are the implications of'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='the implications of this first of all we now know that with Lama 3 open source llms can be used to'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='llms can be used to generate insights and this is very good news for privacy sensitive use cases so'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='use cases so use cases where you want to feed sensitive customer data back to the llm and in real'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"the llm and in real sensitive use cases you probably don't want to use gr Cloud you want to use\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='you want to use Lama 3 locally this is also very good news for query heavy applications so text'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"so text tosql applications can be query heavy if they're rolled out in a big organization so\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"big organization so there's a cost consideration that might be worth looking into now Gro cloud is\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='now Gro cloud is all about realtime gen inference so Lama 3 on gr cloud is going to be really'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='going to be really useful for Consumer facing applications where speed is necessary finally I think'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"finally I think it's pretty clear now that data pipelines dashboards reports and so on will be llm\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='so on will be llm generated in the future and not so distant future so if you are a data analyst or'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='a data analyst or data engineer you should really pay attention to this and learn this new'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content=\"and learn this new technology all right that's it for now if you enjoyed this video I suggest you\"),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='video I suggest you check out one of the other videos on generating SQL with llm chains thanks for'),\n",
       " Document(metadata={'source': 'AOEGOhkGtjI'}, page_content='chains thanks for watching')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_split = text_splitter.split_documents(docs)\n",
    "docs_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN3TsjTe8DC5"
   },
   "source": [
    "### 벡터 스토어 설정\n",
    "분할된 문서를 효율적으로 저장하고 검색하려면 벡터 스토어를 사용할 수 있습니다. 이 예에서는 Redis를 벡터 스토어로 사용합니다. 먼저 Redis 연결을 설정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "cdEOwSbJWlHn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import redis\n",
    "\n",
    "REDIS_HOST=\"redis-18231.c91.us-east-1-3.ec2.redns.redis-cloud.com\"\n",
    "REDIS_PASSWORD=os.environ[\"REDIS_PASSWORD\"]\n",
    "REDIS_PORT=\"18231\"\n",
    "REDIS_URL = \"redis://default:\" + REDIS_PASSWORD + \"@\" + REDIS_HOST + \":\" + REDIS_PORT\n",
    "\n",
    "r = redis.Redis(\n",
    "    host=REDIS_HOST,\n",
    "    port=REDIS_PORT,\n",
    "    password=REDIS_PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAPCRt8NrdFA",
    "outputId": "36a4ecd1-a680-4fdc-8840-f32ec3b5ce5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 연결 확인\n",
    "r.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pEScbmg3riFi",
    "outputId": "d1b6f482-b3aa-4f6e-852c-d572c3e89eec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 선택된 Redis 데이터베이스의 모든 키를 삭제/초기화\n",
    "r.flushdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9r0bST29kET"
   },
   "source": [
    "Hugging Face 임베딩을 사용하여 문서의 벡터 표현을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "G8NrZzTWro9P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPV5hHaTrzs3",
    "outputId": "78b908f1-cb34-4679-cbb8-620aac6e35a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trimu\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf644ccb478f42b8978ffa594ab19e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trimu\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\trimu\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d40715d22f14a02ad456aacc35b93c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f94fafebb740cc97eca9f379448a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f55016b4af542a9bba7994651d9eb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab41a6ff7b6482dbc9b99d037d97615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c205f79054434e148dd8681ffba16e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1124029db2e74f3e94617ede92e83c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19934a2843dc40a999f7765187b62032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74890708e54644b0857febe7f18adfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2faf1f593c4550aa8fda30caec7e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e633326c2141a290785050f845b1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings  = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "iQvzRoDAsSip"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.redis import Redis\n",
    "\n",
    "# Redis 벡터스토어 인스턴스 생성\n",
    "rds = Redis.from_documents(\n",
    "    docs_split,\n",
    "    embeddings,\n",
    "    redis_url=REDIS_URL,\n",
    "    index_name=\"youtube\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_R-1tlh5sgqf",
    "outputId": "8a8055e8-3746-45fe-bed8-7027dbb9b998"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'youtube'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 벡터스토어 인스턴스에서 사용되는 인덱스의 이름\n",
    "rds.index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmS3J8oSuj_0",
    "outputId": "ca18a07a-2d1f-4ef8-865e-43a08c28ffa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RedisVectorStoreRetriever(tags=['Redis', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.redis.base.Redis object at 0x000001F7F6F31FA0>, search_kwargs={'k': 10})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redis 벡터스토어에서 검색을 위한 retriever 객체 생성\n",
    "# search_type: \"similarity\"로 설정하여 유사도 검색을 수행\n",
    "# search_kwargs: 검색 결과에서 상위 10개의 결과를 반환하도록 설정\n",
    "\n",
    "retriever = rds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkWr6Dw2u4xW",
    "outputId": "23aea9bd-f32a-4af3-ffe6-0188a347d66c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'doc:youtube:cce0287777f348129314b680a9a31a6f', 'source': 'AOEGOhkGtjI'}, page_content=\"this in a customer data platform now let's try a different one I'll do a classical one show me the\"),\n",
       " Document(metadata={'id': 'doc:youtube:e99aeb6d5a3b494ba6427b76577ec329', 'source': 'AOEGOhkGtjI'}, page_content='a data analyst or data engineer you should really pay attention to this and learn this new'),\n",
       " Document(metadata={'id': 'doc:youtube:8e3c7cdf698d4ea095e0be9f91819cec', 'source': 'AOEGOhkGtjI'}, page_content='we can then have a look at the data frame and this is essentially an audience that you could use'),\n",
       " Document(metadata={'id': 'doc:youtube:f1a60abf531d41759f234bbe6dcf86dd', 'source': 'AOEGOhkGtjI'}, page_content='discuss some of the implications for llm based data analysis on l.a. comom you can read about Lama'),\n",
       " Document(metadata={'id': 'doc:youtube:b8ef35f485d24b1b86dba324d1a5fbc6', 'source': 'AOEGOhkGtjI'}, page_content='the last video in the dashboard video it is an e-commerce data set with four tables customers'),\n",
       " Document(metadata={'id': 'doc:youtube:52941a4a16c04742bab9c973c2b66ab9', 'source': 'AOEGOhkGtjI'}, page_content=\"going to connect to biy and the data set I'm using is the same data set I used in the last video in\"),\n",
       " Document(metadata={'id': 'doc:youtube:dd03540c92da4028aafba85aba7cc7ff', 'source': 'AOEGOhkGtjI'}, page_content='with the purchase frequency and the average order value so this is very useful so we can use llama'),\n",
       " Document(metadata={'id': 'doc:youtube:c49e819c0a2843c1854eaedb5157f356', 'source': 'AOEGOhkGtjI'}, page_content='to give L three is the following give me a list of the best customers including their rank their'),\n",
       " Document(metadata={'id': 'doc:youtube:0470c6981a5143eebed1d39133ac1a51', 'source': 'AOEGOhkGtjI'}, page_content='when it comes to analyzing data with llms here I have L 3 on gr cloud and not only are the text to'),\n",
       " Document(metadata={'id': 'doc:youtube:9b58e4ddbf45475dba11fdb731415aa3', 'source': 'AOEGOhkGtjI'}, page_content=\"bit crate data set then I'm injecting the main question or the query and finally I'll be injecting\")]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #검색 쿼리로 \"data analysis\"를 사용하여 가장 관련성 높은 상위 10개 문서 청크 반환\n",
    "retriever.invoke(\"data analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2s-x-iRvWrY"
   },
   "source": [
    "## 5. RAG Chain 만들기\n",
    "\n",
    "RAG는 외부 데이터로 대형 언어 모델(LLM)의 지식을 증강하는 기술입니다. RAG 애플리케이션은 두 단계로 구축할 수 있습니다:   \n",
    "   \n",
    "1) 외부 데이터 색인화   \n",
    "2) 검색 및 출력 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "JgOnOVNfvAnO"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "AwMZCi70AmY_"
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    # \"context\"는 입력 데이터에서 'question' 값을 가져오고, 이를 retriever에 전달하여 유사한 문서 검색\n",
    "    {\"context\": (lambda x: x['question']) | retriever,\n",
    "     \"question\": (lambda x: x['question'])}  # \"question\"은 입력 데이터에서 'question' 값을 그대로 반환\n",
    "    | prompt    # 검색된 문서를 이용해 프롬프트를 생성함\n",
    "    | llm_gpt4   # 프롬프트를 기반으로 GPT-4 모델을 사용해 응답을 생성\n",
    "    | StrOutputParser()    # LLM의 출력을 문자열로 파싱\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k56mgJ9Ix_Gt",
    "outputId": "6c6d5ace-7582-4d24-8d86-9f63940b27b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3를 이용하여 데이터 분석, 인사이트 생성, 그리고 LLM 기반의 데이터 분석에 대한 논의를 할 수 있습니다. 또한, 모델의 성능을 극대화하고 다양한 기능을 활용할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "answer = chain.invoke({\"question\": \"Llama3를 이용하여 할 수 있는 일이 뭐야?\"})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1jH-oz68jvv"
   },
   "source": [
    "### 6. Tools / Toolkits\n",
    "\n",
    "- 도구는 에이전트, 체인 또는 대형 언어 모델(LLM)이 세상과 상호 작용할 수 있게 하는 인터페이스입니다.\n",
    "    - Python REPL, Wikipdedia, YouTube, Zapier, Gradio, etc\n",
    "\n",
    "- 툴킷은 특정 작업을 위해 함께 사용하도록 설계된 도구들의 모음입니다.\n",
    "    - Airbyte, Github, Gitlab, Jira, Slack, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgMKjECQAm0S"
   },
   "source": [
    "### 6. Chain with a Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "RkuqKhVpyKxm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet youtube_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZF5O2LeB-vD",
    "outputId": "86cbdc7a-8466-410b-864b-7c67acc519db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.youtube.com/watch?v=AOEGOhkGtjI&pp=ygUNUmFiYml0bWV0cmljcw%3D%3D', 'https://www.youtube.com/watch?v=rOfg-nbkk8A&pp=ygUNUmFiYml0bWV0cmljcw%3D%3D']\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "# YouTubeSearchTool 인스턴스 생성\n",
    "youtube_tool = YouTubeSearchTool()\n",
    "\n",
    "# \"Andrej Karpathy\"라는 키워드로 유튜브 검색 실행\n",
    "results = youtube_tool.run(\"Rabbitmetrics\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5BZ28LTAtul",
    "outputId": "62f913a7-6b6c-415c-d494-4ad127a6a18d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ewQ88czEIZXYwRgv1F0wUJOw', 'function': {'arguments': '{\"query\":\"Rabbitmetrics\"}', 'name': 'youtube_search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 95, 'total_tokens': 110}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f86447bb-b174-4182-9283-1178ae83e229-0', tool_calls=[{'name': 'youtube_search', 'args': {'query': 'Rabbitmetrics'}, 'id': 'call_ewQ88czEIZXYwRgv1F0wUJOw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 95, 'output_tokens': 15, 'total_tokens': 110})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YouTube 도구를 LLM에 바인딩\n",
    "llm_with_tools = llm_gpt4.bind_tools([youtube_tool])\n",
    "\n",
    "msg = llm_with_tools.invoke(\"Rabbitmetrics YT videos\")\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHYOhC0BCPPe",
    "outputId": "7dd07d3c-7316-4d98-849d-5e9e025e67ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'youtube_search',\n",
       "  'args': {'query': 'Rabbitmetrics'},\n",
       "  'id': 'call_ewQ88czEIZXYwRgv1F0wUJOw',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-D_-T27Azsy",
    "outputId": "87aec236-a536-48f7-8af1-bbbc8fd8f656"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001F7866375E0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001F7866495E0>, model_name='gpt-4o-mini-2024-07-18', openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'youtube_search', 'description': 'search for youtube videos associated with a person. the input to this tool should be a comma separated list, the first part contains a person name and the second a number that is the maximum number of video results to return aka num_results. the second part is optional', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string'}}, 'required': ['query']}}}]})\n",
       "| RunnableLambda(lambda x: x.tool_calls[0]['args'])\n",
       "| YouTubeSearchTool()"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM의 출력에서 첫 번째 도구 호출의 \"__arg1\" 인수를 추출\n",
    "# YouTubeSearchTool을 사용하여 추출된 인수로 유튜브 검색 실행\n",
    "chain = llm_with_tools | (lambda x: x.tool_calls[0][\"args\"]) | youtube_tool\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "FFzYcVymCK-y",
    "outputId": "01733b66-3806-4bfe-8c56-a276044c8703"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW4%3D', 'https://www.youtube.com/watch?v=8BV9TW490nQ&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW4%3D']\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"Find some Rabbitmetrics videos on langchain\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDYHTp3LIzr4",
    "outputId": "48b727f3-bd82-4602-ebdd-0a257fa76799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW4%3D',\n",
       " 'https://www.youtube.com/watch?v=8BV9TW490nQ&pp=ygUXUmFiYml0bWV0cmljcyBsYW5nY2hhaW4%3D']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "ast.literal_eval(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwjsaxjQPBdS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
