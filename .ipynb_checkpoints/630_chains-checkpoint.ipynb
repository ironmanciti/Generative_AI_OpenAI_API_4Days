{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itgSZDGeuQIw"
   },
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aUUXRmpPTRxB",
    "outputId": "57325999-71b5-4da9-968c-b8fb7893d155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "python-dotenv \\\n",
    "langchain \\\n",
    "langchain-community \\\n",
    "openai \\\n",
    "anthropic \\\n",
    "langchain-openai \\\n",
    "langchain-anthropic \\\n",
    "langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fap_NphGcFQl",
    "outputId": "1d6a316f-42c4-4c92-da70-3a023a28029e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MyHpEqO6v9Ho"
   },
   "outputs": [],
   "source": [
    "#Model = \"gpt-4o\"\n",
    "Model = \"gpt-3.5-turbo\"\n",
    "\n",
    "model = ChatOpenAI(model=Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hii9ZITqLK3",
    "outputId": "1e1c6693-e7d6-4516-929d-aa4d476b6f3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with System and Human Messages ------\n",
      "\n",
      "messages=[SystemMessage(content='당신은 강아지에 대한 농담을 잘하는 코미디언입니다.'), HumanMessage(content='3 개의 농담을 해 주세요.')]\n",
      "\n",
      "1. 왜 강아지가 항상 꼬리를 흔들깁니까? 그들은 뒤에 있는 것을 보려고 하는 게 아니라, 쓰레기통에서 뭔가를 찾으려고 하는 거에요!\n",
      "\n",
      "2. 강아지가 왜 컴퓨터를 켜놓고 나가있었을까요? 인터넷을 켜놓고 심심해서 '개'글을 검색하러 갔기 때문이에요!\n",
      "\n",
      "3. 어떤 강아지가 가장 좋은 댄서일까요? 댄싱 퀸 퍼피!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt using message\n",
    "messages = [\n",
    "    (\"system\", \"당신은 {topic}에 대한 농담을 잘하는 코미디언입니다.\"),\n",
    "    (\"human\", \"{joke_count} 개의 농담을 해 주세요.\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"강아지\", \"joke_count\": 3})\n",
    "\n",
    "print(\"\\n----- Prompt with System and Human Messages ------\\n\")\n",
    "print(prompt)\n",
    "print()\n",
    "result = model.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxjSPugFqOVW"
   },
   "source": [
    "- chain을 구성하여 도일한 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o03iGReZBTKS",
    "outputId": "8ec482fe-7c7f-4e22-c867-9912c60c8071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 왜 강아지는 항상 꼬리를 흔들깁니까? 왜냐하면 꼬리가 웃긴 이야기를 들었기 때문이죠!\n",
      "\n",
      "2. 강아지가 왜 컴퓨터를 잃어버렸을까요? 왜냐하면 계속 '모니터'를 따라다니다가 '마우스'를 물었기 때문이죠!\n",
      "\n",
      "3. 강아지가 왜 책을 읽지 않을까요? 왜냐하면 그들에겐 이미 '꼬리'가 있어서 페이지를 넘길 필요가 없기 때문이죠!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LCEL (LangChain Expression Language)를 이용하여 chain 결합\n",
    "chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "result = chain.invoke({\"topic\": \"강아지\", \"joke_count\": 3})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dncbO3h4slF8"
   },
   "source": [
    "- chain 의 각 단계를 분해하면 내부적으로 다음과 같은 RunnableLambda 로 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_HD83Z6mjIv",
    "outputId": "67c8eaa2-3ea3-4377-e474-105265a42cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 왜 강아지가 컴퓨터를 사용하지 않을까요? 털때문에 마우스가 안 보이거든요!\n",
      "2. 강아지가 식당을 운영하면 무슨 음식을 팔까요? 개구리 튀김이겠죠!\n",
      "3. 강아지가 서점을 운영하면 어떤 책을 팔까요? \"왈왈과 평화\"!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "prompt_formatting = RunnableLambda(lambda x: prompt_template.format(**x)) # dictionary 입력\n",
    "model_invoke = RunnableLambda(lambda x: model.invoke(x))  # prompt 이용하여 model API 호출\n",
    "output_parsing = RunnableLambda(lambda x: x.content)  # API response 중 content 추출\n",
    "\n",
    "# chain 구성\n",
    "chain = RunnableSequence(first=prompt_formatting, middle=[model_invoke], last=output_parsing)\n",
    "\n",
    "result = chain.invoke({\"topic\": \"강아지\", \"joke_count\": 3})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o-tORmI7mz7"
   },
   "source": [
    "- RunnableLambda를 이용한 추가적인 processing 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48JctDMjOHhV",
    "outputId": "1408d153-aecc-4f86-89bf-afe69f984436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 길이: 39\n",
      "\n",
      "1. 왜 강아지가 신문을 읽지 않을까요? 뉴스가 너무 따분해서 꼬리를 흔들 수 없기 때문이죠!\n",
      "\n",
      "2. 강아지가 왜 신나게 짖을까요? 왜냐하면 그들이 '보고' 싶어서 그래요!\n",
      "\n",
      "3. 강아지가 왜 컴퓨터를 사용하지 않을까요? 모니터를 보면서 \"왈왈\" 하다 보면 화면이 모두 물들어 버리니까요!\n"
     ]
    }
   ],
   "source": [
    "uppercase_change = RunnableLambda(lambda x: x.upper())\n",
    "word_count = RunnableLambda(lambda x: f\"단어 길이: {len(x.split())}\\n\\n{x}\")\n",
    "\n",
    "chain = prompt_template | model | StrOutputParser() | uppercase_change | word_count\n",
    "\n",
    "result = chain.invoke({\"topic\": \"강아지\", \"joke_count\": 3})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xHSulHyPC0A"
   },
   "source": [
    "# 병렬처리 Workflow\n",
    "\n",
    "이 워크플로우는 특정 제품에 대해 기능 목록을 생성하고 이를 분석하는 과정을 보여줍니다.\n",
    "\n",
    "## 1. Prompt\n",
    "- **역할**: 제품의 이름을 입력받습니다.\n",
    "- **입력**: `product_name`\n",
    "- **출력**: 제품의 기능 목록을 생성하기 위한 프롬프트를 생성합니다.\n",
    "\n",
    "## 2. Model\n",
    "- **역할**: 입력된 프롬프트를 기반으로 제품의 특성 목록을 생성합니다.\n",
    "- **입력**: 프롬프트\n",
    "- **출력**: 제품의 특성 목록\n",
    "\n",
    "## 3. Str Parser\n",
    "- **역할**: 모델의 출력을 파싱하여 필요한 정보를 추출합니다.\n",
    "- **입력**: 모델의 출력\n",
    "- **출력**: 파싱된 내용의 content\n",
    "\n",
    "## 4. 분석 단계\n",
    "### Pros\n",
    "- **역할**: 장점 목록을 생성합니다.\n",
    "- **입력**: 파싱된 내용\n",
    "- **출력**: 장점 목록의 content\n",
    "\n",
    "### Cons\n",
    "- **역할**: 단점 목록을 생성합니다.\n",
    "- **입력**: 파싱된 내용\n",
    "- **출력**: 단점 목록의 content\n",
    "\n",
    "### 분석 결과\n",
    "- **분석**: 장점과 단점을 Combine 하여 최종 결과를 print 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghgo4M6oSLtf",
    "outputId": "3ddb13e3-1cb5-4d8c-c554-e625e72ecd0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final output {'branches': {'pros': 'MacBook Pro의 위주 특성들은 다음과 같은 장점들을 가지고 있습니다:\\n\\n1. 고성능 프로세서: 강력한 Intel 프로세서를 사용하여 뛰어난 성능을 제공하여 복잡한 작업도 빠르고 효율적으로 처리할 수 있습니다.\\n2. Retina 디스플레이: 고해상도 Retina 디스플레이를 통해 선명하고 생생한 화질을 제공하여 사용자들에게 시각적으로 뛰어난 경험을 제공합니다.\\n3. 우수한 배터리 수명: 긴 배터리 수명은 사용자들이 장시간 사용할 수 있게 해주어 이동 중에도 높은 편의성을 제공합니다.\\n4. 편리한 운영체제: macOS 운영체제는 직관적이고 사용하기 쉬운 인터페이스를 제공하여 사용자들이 빠르게 작업을 수행할 수 있습니다.\\n5. 소형 경량 디자인: 슬림하고 가벼운 디자인으로 휴대하기 편리하며 이동 중에도 불편함 없이 사용할 수 있습니다.\\n6. 탁월한 성능: 그래픽 처리, 멀티태스킹, 비디오 편집 등 다양한 작업을 원활하게 처리할 수 있어 전문가들과 창작자들에게 적합한 선택지가 됩니다.', 'cons': 'MacBook Pro의 중요한 특성들에는 많은 장점이 있지만 몇 가지 단점도 고려해야 합니다. 이러한 단점은 다음과 같습니다:\\n\\n1. 높은 가격: MacBook Pro는 다른 노트북에 비해 상대적으로 높은 가격대에 속합니다.\\n2. 확장성 제한: 일부 모델의 MacBook Pro는 사용자가 하드웨어를 업그레이드하기 어려울 수 있습니다.\\n3. 포트 부족: 최신 모델의 MacBook Pro는 USB 포트 및 SD 카드 슬롯과 같은 기본적인 포트가 부족할 수 있습니다.\\n4. 업그레이드 가능성 제한: 일부 모델의 MacBook Pro는 램이나 저장 용량을 사용자가 직접 업그레이드하기 어렵거나 불가능할 수 있습니다.\\n5. 소프트웨어 호환성: 일부 사용자는 macOS 운영체제가 Windows 운영체제에 비해 일부 소프트웨어와 게임의 호환성이 떨어질 수 있다고 느낄 수 있습니다.'}}\n",
      "Pros:\n",
      "MacBook Pro의 위주 특성들은 다음과 같은 장점들을 가지고 있습니다:\n",
      "\n",
      "1. 고성능 프로세서: 강력한 Intel 프로세서를 사용하여 뛰어난 성능을 제공하여 복잡한 작업도 빠르고 효율적으로 처리할 수 있습니다.\n",
      "2. Retina 디스플레이: 고해상도 Retina 디스플레이를 통해 선명하고 생생한 화질을 제공하여 사용자들에게 시각적으로 뛰어난 경험을 제공합니다.\n",
      "3. 우수한 배터리 수명: 긴 배터리 수명은 사용자들이 장시간 사용할 수 있게 해주어 이동 중에도 높은 편의성을 제공합니다.\n",
      "4. 편리한 운영체제: macOS 운영체제는 직관적이고 사용하기 쉬운 인터페이스를 제공하여 사용자들이 빠르게 작업을 수행할 수 있습니다.\n",
      "5. 소형 경량 디자인: 슬림하고 가벼운 디자인으로 휴대하기 편리하며 이동 중에도 불편함 없이 사용할 수 있습니다.\n",
      "6. 탁월한 성능: 그래픽 처리, 멀티태스킹, 비디오 편집 등 다양한 작업을 원활하게 처리할 수 있어 전문가들과 창작자들에게 적합한 선택지가 됩니다.\n",
      "\n",
      "Cons:\n",
      "MacBook Pro의 중요한 특성들에는 많은 장점이 있지만 몇 가지 단점도 고려해야 합니다. 이러한 단점은 다음과 같습니다:\n",
      "\n",
      "1. 높은 가격: MacBook Pro는 다른 노트북에 비해 상대적으로 높은 가격대에 속합니다.\n",
      "2. 확장성 제한: 일부 모델의 MacBook Pro는 사용자가 하드웨어를 업그레이드하기 어려울 수 있습니다.\n",
      "3. 포트 부족: 최신 모델의 MacBook Pro는 USB 포트 및 SD 카드 슬롯과 같은 기본적인 포트가 부족할 수 있습니다.\n",
      "4. 업그레이드 가능성 제한: 일부 모델의 MacBook Pro는 램이나 저장 용량을 사용자가 직접 업그레이드하기 어렵거나 불가능할 수 있습니다.\n",
      "5. 소프트웨어 호환성: 일부 사용자는 macOS 운영체제가 Windows 운영체제에 비해 일부 소프트웨어와 게임의 호환성이 떨어질 수 있다고 느낄 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 상품 리뷰 전문가 입니다.\"),\n",
    "        (\"human\", \"{product_name} 상품의 중요 특성을 나열해 주세요.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def analyze_pros(features):\n",
    "    pros_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "           (\"system\", \"당신은 상품 리뷰 전문가 입니다.\"),\n",
    "           (\"human\",\n",
    "            \"특성들은 다음과 같습니다. {features}. 이 특성들의 장점을 나열해 주세요.\")\n",
    "        ]\n",
    "    )\n",
    "    return pros_template.format_prompt(features=features)\n",
    "\n",
    "def analyze_cons(features):\n",
    "    cons_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "           (\"system\", \"당신은 상품 리뷰 전문가 입니다.\"),\n",
    "           (\"human\",\n",
    "            \"특성들은 다음과 같습니다. {features}. 이 특성들의 단점을 나열해 주세요.\")\n",
    "        ]\n",
    "    )\n",
    "    return cons_template.format_prompt(features=features)\n",
    "\n",
    "def combine_pros_cons(pros, cons):\n",
    "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
    "\n",
    "# LCEL 을 이용한 .branch 단순화\n",
    "pros_branch_chain = RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
    "\n",
    "cons_branch_chain = RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
    "\n",
    "chain = (\n",
    "    prompt_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
    "    | RunnableLambda(lambda x: print(\"final output\", x) or combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
